{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61139a47",
   "metadata": {},
   "source": [
    "# Forest Loss Driver Analysis (Northern Maine Acaduab Reguib)\n",
    "\n",
    "**Goal:** Use a Random Forest model to identify the relative importance of\n",
    "preceding spectral conditions and climate variables in predicting forest loss\n",
    "events (from Hansen GFC) between 2001 and 2021.\n",
    "\n",
    "**Data:**\n",
    "- Annual Landsat C02 L2 median composites (2000-2021), Bands: B, G, R, NIR, SWIR1, SWIR2, NDVI, NBR\n",
    "- Hansen GFC Loss Year (2000-2021 v1.9)\n",
    "- Hansen GFC Tree Cover 2000\n",
    "- Annual TerraClimate: Mean Temperature, Total Precipitation (2000-2021)\n",
    "- SRTM DEM & Slope\n",
    "\n",
    "**Methodology:**\n",
    "1. Load and verify data alignment (CRS, Extent, Resolution).\n",
    "2. Define sampling strategy: Sample loss pixels and non-loss pixels (within forest mask) annually.\n",
    "3. Extract features for sampled pixels:\n",
    "   - Target: Loss in year `t` (binary)\n",
    "   - Features: Landsat(t-1), Climate(t-1), Climate(t), DEM, Slope\n",
    "4. Split data temporally (e.g., train 2001-2015, test 2016-2021).\n",
    "5. Train Random Forest Classifier.\n",
    "6. Evaluate model performance.\n",
    "7. Analyze feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938ce21",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4d5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time # Keep track of processing time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.enums import Resampling # if needed for consistency checks\n",
    "# import geopandas as gpd # If needed for vector operations later\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.utils import shuffle # For sampling\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib # To save the trained model\n",
    "\n",
    "# Optional: for parallel processing if sampling is slow\n",
    "# import dask.array as da\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01896d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e84f24a",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to use Project Root: /Users/benjaminpace/MLCS/mlcs\n",
      "Project Root confirmed: /Users/benjaminpace/MLCS/mlcs\n",
      "Data Directory: /Users/benjaminpace/MLCS/mlcs/data\n",
      "Landsat Directory: /Users/benjaminpace/MLCS/mlcs/data/landsat\n",
      "Hansen Directory: /Users/benjaminpace/MLCS/mlcs/data/hansen\n",
      "Climate Directory: /Users/benjaminpace/MLCS/mlcs/data/climate\n",
      "Auxiliary Directory: /Users/benjaminpace/MLCS/mlcs/data/aux\n",
      "Output Directory: /Users/benjaminpace/MLCS/mlcs/output\n",
      "Analysis Target Years (YEARS): [2001, 2002, 2003]\n",
      "Expected Landsat Bands (in order): ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'NDVI', 'NBR']\n",
      "Climate Variables: ['mean_temp', 'total_precip']\n",
      "Static Variables: ['dem', 'slope']\n",
      "Test Split Year (targets < this year for train): 2003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Project Structure ---\n",
    "# IMPORTANT: Verify this PROJECT_ROOT path is correct for your system.\n",
    "# It should be the main folder containing 'data/', 'notebooks/', etc.\n",
    "PROJECT_ROOT = Path('/Users/benjaminpace/MLCS/mlcs') # <--- ADJUST THIS PATH IF YOUR PROJECT IS ELSEWHERE\n",
    "\n",
    "# Print the path to be sure, then you can comment it out\n",
    "print(f\"Attempting to use Project Root: {PROJECT_ROOT}\")\n",
    "if not (PROJECT_ROOT / 'data').exists():\n",
    "    # Try to infer from current working directory if notebook is in 'notebooks/'\n",
    "    current_dir = Path(os.getcwd())\n",
    "    if current_dir.name == 'notebooks' and (current_dir.parent / 'data').exists():\n",
    "        PROJECT_ROOT = current_dir.parent\n",
    "        print(f\"Adjusted Project Root (inferred): {PROJECT_ROOT}\")\n",
    "    else:\n",
    "        # If still not found, raise a clear error.\n",
    "        raise FileNotFoundError(\n",
    "            f\"CRITICAL: 'data' directory not found relative to an assumed Project Root of {PROJECT_ROOT}. \"\n",
    "            f\"Please verify the PROJECT_ROOT variable in this cell. Current directory is {os.getcwd()}.\"\n",
    "        )\n",
    "else:\n",
    "    print(f\"Project Root confirmed: {PROJECT_ROOT}\")\n",
    "\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "LANDSAT_DIR = DATA_DIR / 'landsat'\n",
    "HANSEN_DIR = DATA_DIR / 'hansen'\n",
    "CLIMATE_DIR = DATA_DIR / 'climate'\n",
    "AUX_DIR = DATA_DIR / 'aux'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True) # Create output directory if it doesn't exist\n",
    "\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Landsat Directory: {LANDSAT_DIR}\")\n",
    "print(f\"Hansen Directory: {HANSEN_DIR}\")\n",
    "print(f\"Climate Directory: {CLIMATE_DIR}\")\n",
    "print(f\"Auxiliary Directory: {AUX_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# --- Data File Paths (These define the expected names within the subdirectories) ---\n",
    "HANSEN_LOSS_YEAR_PATH = HANSEN_DIR / 'hansen_lossyear_2000_2021.tif'\n",
    "HANSEN_COVER_2000_PATH = HANSEN_DIR / 'hansen_treecover_2000.tif'\n",
    "DEM_PATH = AUX_DIR / 'dem.tif'\n",
    "SLOPE_PATH = AUX_DIR / 'slope.tif'\n",
    "\n",
    "# --- Analysis Parameters (ULTRA MINIMAL for 6-hour sprint) ---\n",
    "START_YEAR = 2001 # First year for which we predict loss (uses t-1 predictors from 2000)\n",
    "END_YEAR = 2003     # Last target year for loss. Targets: 2001, 2002, 2003.\n",
    "                    # This means Landsat predictors needed up to 2002.\n",
    "                    # Climate predictors needed up to 2002 (for t-1) and 2003 (for t).\n",
    "YEARS = list(range(START_YEAR, END_YEAR + 1)) # Python range goes up to, but not including, the stop value.\n",
    "print(f\"Analysis Target Years (YEARS): {YEARS}\") # Should be [2001, 2002, 2003]\n",
    "\n",
    "# Define the bands we expect in the Landsat composites AND THEIR ORDER\n",
    "# This order MUST match the order of bands in your GeoTIFF files.\n",
    "LANDSAT_BANDS = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'NDVI', 'NBR']\n",
    "print(f\"Expected Landsat Bands (in order): {LANDSAT_BANDS}\")\n",
    "\n",
    "CLIMATE_VARS = ['mean_temp', 'total_precip']\n",
    "print(f\"Climate Variables: {CLIMATE_VARS}\")\n",
    "\n",
    "STATIC_VARS = ['dem', 'slope']\n",
    "print(f\"Static Variables: {STATIC_VARS}\")\n",
    "\n",
    "# --- Sampling Parameters ---\n",
    "NON_LOSS_RATIO = 1      # 1 non-loss point for every 1 loss point (minimal for speed)\n",
    "MIN_TREE_COVER = 30     # Minimum tree cover in 2000 to be considered 'forest'\n",
    "RANDOM_STATE = 42       # For reproducibility\n",
    "\n",
    "# --- Modeling Parameters (ULTRA MINIMAL for speed) ---\n",
    "TEST_SPLIT_YEAR = 2003   # Train on target years < 2003 (i.e., 2001, 2002)\n",
    "                         # Test on target years >= 2003 (i.e., 2003)\n",
    "print(f\"Test Split Year (targets < this year for train): {TEST_SPLIT_YEAR}\")\n",
    "\n",
    "RF_N_ESTIMATORS = 10    # Very few trees\n",
    "RF_MAX_DEPTH = 5        # Very shallow trees\n",
    "RF_MIN_SAMPLES_LEAF = 50 # Leaves must have at least this many samples\n",
    "RF_MIN_SAMPLES_SPLIT = 100 # Nodes must have at least this many samples to split\n",
    "RF_N_JOBS = -1          # Use all available CPU cores (good for RF)\n",
    "# class_weight='balanced' will be set in the classifier instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93418285",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Verification\n",
    "\n",
    "**CRITICAL STEP:** Verify that all input rasters have the *exact same* Coordinate Reference System (CRS), transform (affine), dimensions (width, height), and resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "646c0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Cell 7 of your notebook (verify_raster_alignment function definition)\n",
    "\n",
    "import rasterio # Ensure rasterio is imported here if not globally in Cell 3\n",
    "from pathlib import Path # Ensure Path is imported\n",
    "\n",
    "def verify_raster_alignment(raster_paths):\n",
    "    \"\"\"Checks CRS, transform, shape of multiple rasters.\"\"\"\n",
    "    print(\"Verifying raster alignment...\")\n",
    "    if not raster_paths:\n",
    "        print(\"  No raster paths provided for verification.\")\n",
    "        return None\n",
    "\n",
    "    reference_profile = None\n",
    "    all_aligned = True # Assume aligned until a mismatch is found\n",
    "    checked_paths_count = 0\n",
    "\n",
    "    for path_obj in raster_paths:\n",
    "        path_str = str(path_obj) # Ensure it's a string for rasterio.open\n",
    "        current_file_path = Path(path_str)\n",
    "\n",
    "        if not current_file_path.exists():\n",
    "            print(f\"  WARNING: File not found, skipping: {current_file_path.name}\")\n",
    "            # If a file is missing, we can't confirm alignment with it.\n",
    "            # Depending on strictness, you might want to set all_aligned = False here.\n",
    "            # For now, we'll just skip it and report issues with files that *do* exist.\n",
    "            continue\n",
    "\n",
    "        checked_paths_count += 1\n",
    "        try:\n",
    "            with rasterio.open(current_file_path) as src:\n",
    "                # Get dtype of the first band as representative for the file\n",
    "                # Note: Rasterio datasets can have bands of different dtypes,\n",
    "                # but for our GEE exports, they should be consistent within a file.\n",
    "                current_dtype = src.dtypes[0] # dtypes is a tuple of band dtypes\n",
    "\n",
    "                profile = {\n",
    "                    'path_name': current_file_path.name, # Store only name for cleaner printing\n",
    "                    'crs': src.crs,\n",
    "                    'transform': src.transform,\n",
    "                    'width': src.width,\n",
    "                    'height': src.height,\n",
    "                    'count': src.count,         # Number of bands\n",
    "                    'dtype_band1': current_dtype # Data type of the first band\n",
    "                }\n",
    "                print(f\"--- Checking: {profile['path_name']} ---\")\n",
    "                # print(f\"  CRS: {profile['crs']}\")\n",
    "                # print(f\"  Transform: {profile['transform']}\")\n",
    "                # print(f\"  Shape: ({profile['height']}, {profile['width']})\")\n",
    "                # print(f\"  Band Count: {profile['count']}\")\n",
    "                # print(f\"  Dtype (Band 1): {profile['dtype_band1']}\")\n",
    "\n",
    "\n",
    "                if reference_profile is None: # This is the first valid file encountered\n",
    "                    reference_profile = profile\n",
    "                    print(f\"  Set as Reference: CRS={profile['crs']}, Shape=({profile['height']},{profile['width']}), Transform={profile['transform']}\")\n",
    "                else:\n",
    "                    # Compare current file's profile to the reference profile\n",
    "                    if profile['crs'] != reference_profile['crs']:\n",
    "                        print(f\"  MISMATCH: CRS ({profile['crs']}) differs from reference ({reference_profile['crs']})\")\n",
    "                        all_aligned = False\n",
    "                    if profile['transform'] != reference_profile['transform']:\n",
    "                        # Comparing transforms can be tricky due to float precision.\n",
    "                        # A more robust check might compare elements with a tolerance.\n",
    "                        # For now, direct comparison:\n",
    "                        # print(f\"    Current Transform: {profile['transform']}\")\n",
    "                        # print(f\"    Reference Transform: {reference_profile['transform']}\")\n",
    "                        # Check if they are \"close enough\" if direct equality fails due to precision\n",
    "                        if not profile['transform'].almost_equals(reference_profile['transform']):\n",
    "                             print(f\"  MISMATCH: Transform differs significantly from reference.\")\n",
    "                             all_aligned = False\n",
    "                    if profile['width'] != reference_profile['width'] or profile['height'] != reference_profile['height']:\n",
    "                        print(f\"  MISMATCH: Shape ({profile['height']},{profile['width']}) differs from reference ({reference_profile['height']},{reference_profile['width']})\")\n",
    "                        all_aligned = False\n",
    "                    # We don't strictly need to check band count or dtype for *alignment*,\n",
    "                    # but they are good to be aware of. The band count will be checked later.\n",
    "                    # Dtype consistency is good, but not an alignment blocker if CRS/Transform/Shape match.\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR reading or processing {current_file_path.name}: {type(e).__name__} - {e}\")\n",
    "            all_aligned = False # Treat any read error as an alignment failure for that file\n",
    "\n",
    "    if checked_paths_count == 0:\n",
    "        print(\"\\nERROR: No files were found or could be opened for verification.\")\n",
    "        return None\n",
    "\n",
    "    if reference_profile is None:\n",
    "        print(\"\\nERROR: No valid reference raster could be established (all checked files had errors or were missing).\")\n",
    "        return None\n",
    "\n",
    "    if all_aligned:\n",
    "        print(f\"\\nSUCCESS: All {checked_paths_count} checked rasters appear ALIGNED with the reference '{reference_profile['path_name']}'.\")\n",
    "        # Return the common profile derived from the reference (excluding its own path name)\n",
    "        return {key: value for key, value in reference_profile.items() if key != 'path_name'}\n",
    "    else:\n",
    "        print(\"\\nERROR: Raster alignment check FAILED for one or more files. Please review MISMATCH messages above.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e50dd532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to be checked for alignment (ensure these exist in your project structure):\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/hansen/hansen_lossyear_2000_2021.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/hansen/hansen_treecover_2000.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/aux/dem.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/aux/slope.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/landsat/landsat_composite_2000.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/landsat/landsat_composite_2001.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/landsat/landsat_composite_2002.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/mean_temp_2000.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/total_precip_2000.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/mean_temp_2001.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/total_precip_2001.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/mean_temp_2002.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/total_precip_2002.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/mean_temp_2003.tif (Exists: True)\n",
      "  - /Users/benjaminpace/MLCS/mlcs/data/climate/total_precip_2003.tif (Exists: True)\n",
      "\n",
      "All listed files exist. Proceeding with alignment check...\n",
      "Verifying raster alignment...\n",
      "--- Checking: hansen_lossyear_2000_2021.tif ---\n",
      "  Set as Reference: CRS=EPSG:32619, Shape=(10407,11824), Transform=| 30.00, 0.00, 302940.00|\n",
      "| 0.00,-30.00, 5297100.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "--- Checking: hansen_treecover_2000.tif ---\n",
      "--- Checking: dem.tif ---\n",
      "--- Checking: slope.tif ---\n",
      "--- Checking: landsat_composite_2000.tif ---\n",
      "--- Checking: landsat_composite_2001.tif ---\n",
      "--- Checking: landsat_composite_2002.tif ---\n",
      "--- Checking: mean_temp_2000.tif ---\n",
      "--- Checking: total_precip_2000.tif ---\n",
      "--- Checking: mean_temp_2001.tif ---\n",
      "--- Checking: total_precip_2001.tif ---\n",
      "--- Checking: mean_temp_2002.tif ---\n",
      "--- Checking: total_precip_2002.tif ---\n",
      "--- Checking: mean_temp_2003.tif ---\n",
      "--- Checking: total_precip_2003.tif ---\n",
      "\n",
      "SUCCESS: All 15 checked rasters appear ALIGNED with the reference 'hansen_lossyear_2000_2021.tif'.\n",
      "\n",
      "Common Raster Shape: (10407, 11824)\n",
      "Common CRS: EPSG:32619\n",
      "Common Transform: | 30.00, 0.00, 302940.00|\n",
      "| 0.00,-30.00, 5297100.00|\n",
      "| 0.00, 0.00, 1.00|\n",
      "\n",
      "Verifying bands for: landsat_composite_2000.tif\n",
      "  Number of bands found in file: 8\n",
      "  Expected number of bands (from LANDSAT_BANDS variable): 8\n"
     ]
    }
   ],
   "source": [
    "# In Cell 8 of your notebook (Define files_to_check and call verify_raster_alignment)\n",
    "\n",
    "# --- Gather files to check for the ULTRA-MINIMAL run ---\n",
    "# These are the specific files that your analysis (with END_YEAR=2003) will touch\n",
    "# for feature extraction. All of them must exist and be aligned.\n",
    "\n",
    "files_to_check = [\n",
    "    # Base Layers\n",
    "    HANSEN_LOSS_YEAR_PATH,      # Used for sampling targets across all years\n",
    "    HANSEN_COVER_2000_PATH,     # Used for forest mask in sampling\n",
    "    DEM_PATH,                   # Static predictor\n",
    "    SLOPE_PATH,                 # Static predictor\n",
    "\n",
    "    # Landsat Predictors (t-1):\n",
    "    # For target_year 2001, predictor is Landsat 2000\n",
    "    # For target_year 2002, predictor is Landsat 2001\n",
    "    # For target_year 2003, predictor is Landsat 2002\n",
    "    LANDSAT_DIR / 'landsat_composite_2000.tif',\n",
    "    LANDSAT_DIR / 'landsat_composite_2001.tif',\n",
    "    LANDSAT_DIR / 'landsat_composite_2002.tif',\n",
    "\n",
    "    # Climate Predictors (t-1) and Concurrent Climate (t):\n",
    "    # Year 2000 (t-1 for target 2001)\n",
    "    CLIMATE_DIR / 'mean_temp_2000.tif',\n",
    "    CLIMATE_DIR / 'total_precip_2000.tif',\n",
    "    # Year 2001 (t-1 for target 2002; t for target 2001)\n",
    "    CLIMATE_DIR / 'mean_temp_2001.tif',\n",
    "    CLIMATE_DIR / 'total_precip_2001.tif',\n",
    "    # Year 2002 (t-1 for target 2003; t for target 2002)\n",
    "    CLIMATE_DIR / 'mean_temp_2002.tif',\n",
    "    CLIMATE_DIR / 'total_precip_2002.tif',\n",
    "    # Year 2003 (t for target 2003)\n",
    "    CLIMATE_DIR / 'mean_temp_2003.tif',\n",
    "    CLIMATE_DIR / 'total_precip_2003.tif',\n",
    "]\n",
    "\n",
    "# --- Filter out paths for any datasets you decided to EXCLUDE due to earlier issues ---\n",
    "# For example, if DEM was misaligned and you decided to drop it for this run:\n",
    "# files_to_check = [p for p in files_to_check if p != DEM_PATH]\n",
    "# And ensure 'dem' was removed from STATIC_VARS in Cell 5.\n",
    "# For now, this script assumes all listed files are intended to be used.\n",
    "\n",
    "print(\"Files to be checked for alignment (ensure these exist in your project structure):\")\n",
    "all_files_exist = True\n",
    "for f_path in files_to_check:\n",
    "    exists = Path(f_path).exists()\n",
    "    print(f\"  - {f_path} (Exists: {exists})\")\n",
    "    if not exists:\n",
    "        all_files_exist = False\n",
    "\n",
    "if not all_files_exist:\n",
    "    raise FileNotFoundError(\n",
    "        \"CRITICAL: One or more files listed in 'files_to_check' do not exist at the specified path. \"\n",
    "        \"Verify your data movement and file naming.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nAll listed files exist. Proceeding with alignment check...\")\n",
    "\n",
    "# --- Run the alignment check ---\n",
    "common_profile = verify_raster_alignment(files_to_check) # Function defined in Cell 7\n",
    "\n",
    "assert common_profile is not None, \\\n",
    "    \"CRITICAL: Raster alignment check failed. Stopping execution. \" \\\n",
    "    \"Review errors printed by 'verify_raster_alignment' above. \" \\\n",
    "    \"All listed files must be perfectly aligned (CRS, Transform, Dimensions).\"\n",
    "\n",
    "# --- Store key dimensions globally if check passed ---\n",
    "RASTER_HEIGHT = common_profile['height']\n",
    "RASTER_WIDTH = common_profile['width']\n",
    "RASTER_TRANSFORM = common_profile['transform']\n",
    "RASTER_CRS = common_profile['crs']\n",
    "\n",
    "print(f\"\\nCommon Raster Shape: ({RASTER_HEIGHT}, {RASTER_WIDTH})\")\n",
    "print(f\"Common CRS: {RASTER_CRS}\")\n",
    "print(f\"Common Transform: {RASTER_TRANSFORM}\")\n",
    "\n",
    "# --- Quick check on Landsat band count and descriptions (if available) ---\n",
    "# This helps verify if LANDSAT_BANDS order matches the file.\n",
    "try:\n",
    "    # Check the first Landsat file in our list (should be 2000.tif)\n",
    "    landsat_to_check_bands = LANDSAT_DIR / 'landsat_composite_2000.tif' # Or any in files_to_check\n",
    "    if landsat_to_check_bands.exists():\n",
    "        with rasterio.open(landsat_to_check_bands) as src:\n",
    "            print(f\"\\nVerifying bands for: {landsat_to_check_bands.name}\")\n",
    "            print(f\"  Number of bands found in file: {src.count}\")\n",
    "            print(f\"  Expected number of bands (from LANDSAT_BANDS variable): {len(LANDSAT_BANDS)}\")\n",
    "            if src.count != len(LANDSAT_BANDS):\n",
    "                print(f\"  WARNING: Band count mismatch! Code expects {len(LANDSAT_BANDS)} based on LANDSAT_BANDS list.\")\n",
    "            # Band descriptions are often not set by GEE, but if they were, they'd be useful.\n",
    "            # print(f\"  Band descriptions from file: {src.descriptions}\")\n",
    "            # If src.descriptions is like (None, None, ...), GEE didn't set them, which is common.\n",
    "    else:\n",
    "        print(f\"WARNING: Cannot check bands, {landsat_to_check_bands.name} not found for this check.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not perform Landsat band count/description check: {type(e).__name__} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b99d20",
   "metadata": {},
   "source": [
    "## 4. Sampling Strategy\n",
    "\n",
    "For each year `t` from 2001 to 2021:\n",
    "1. Identify pixels where loss occurred exactly in year `t`.\n",
    "2. Identify pixels that are potential non-loss candidates (forested in 2000, no loss 2001-2021).\n",
    "3. Sample loss pixels.\n",
    "4. Sample `NON_LOSS_RATIO` times as many non-loss pixels randomly from the candidates.\n",
    "5. Store pixel coordinates (row, col) and associated target year/loss status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66277eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pixel sampling...\n",
      "Loading base data for sampling (Hansen Loss Year and Cover 2000)...\n",
      "  Loaded Loss Year data (10407x11824)\n",
      "  Loaded Tree Cover 2000 data\n",
      "Found 83160460 potential non-loss candidate pixels.\n",
      "\n",
      "--- Sampling for target year: 2001 ---\n",
      "  Found 705266 loss pixels.\n",
      "  Sampling 705266 non-loss pixels (NON_LOSS_RATIO=1).\n",
      "  Finished sampling for 2001. Points this year: 1410532. Cumulative: 1410532. Time: 2.62s\n",
      "\n",
      "--- Sampling for target year: 2002 ---\n",
      "  Found 716421 loss pixels.\n",
      "  Sampling 716421 non-loss pixels (NON_LOSS_RATIO=1).\n",
      "  Finished sampling for 2002. Points this year: 1432842. Cumulative: 2843374. Time: 2.35s\n",
      "\n",
      "--- Sampling for target year: 2003 ---\n",
      "  Found 385892 loss pixels.\n",
      "  Sampling 385892 non-loss pixels (NON_LOSS_RATIO=1).\n",
      "  Finished sampling for 2003. Points this year: 771784. Cumulative: 3615158. Time: 2.38s\n",
      "\n",
      "Total points sampled across all years (before any subsampling): 3615158\n",
      "Sampled loss counts (before any subsampling):\n",
      "0    1807579\n",
      "1    1807579\n",
      "Name: count, dtype: int64\n",
      "Sampling finished. Total time: 10.11s\n"
     ]
    }
   ],
   "source": [
    "all_sampled_points = [] # Initialize or clear if re-running\n",
    "\n",
    "print(\"Starting pixel sampling...\")\n",
    "sampling_start_time = time.time()\n",
    "\n",
    "# These should be defined from Cell 5 (Configuration)\n",
    "# HANSEN_LOSS_YEAR_PATH, HANSEN_COVER_2000_PATH\n",
    "# RASTER_HEIGHT, RASTER_WIDTH (from Cell 8 common_profile)\n",
    "# MIN_TREE_COVER, YEARS, NON_LOSS_RATIO, RANDOM_STATE\n",
    "\n",
    "try:\n",
    "    print(\"Loading base data for sampling (Hansen Loss Year and Cover 2000)...\")\n",
    "    with rasterio.open(HANSEN_LOSS_YEAR_PATH) as loss_src:\n",
    "        # Verify shape matches common profile before reading\n",
    "        assert (loss_src.height, loss_src.width) == (RASTER_HEIGHT, RASTER_WIDTH), \\\n",
    "            f\"Loss year shape ({loss_src.height},{loss_src.width}) mismatch with common profile ({RASTER_HEIGHT},{RASTER_WIDTH})!\"\n",
    "        loss_year_data = loss_src.read(1)\n",
    "        print(f\"  Loaded Loss Year data ({loss_src.height}x{loss_src.width})\")\n",
    "\n",
    "    with rasterio.open(HANSEN_COVER_2000_PATH) as cover_src:\n",
    "        assert (cover_src.height, cover_src.width) == (RASTER_HEIGHT, RASTER_WIDTH), \\\n",
    "            f\"Cover 2000 shape ({cover_src.height},{cover_src.width}) mismatch with common profile ({RASTER_HEIGHT},{RASTER_WIDTH})!\"\n",
    "        cover_2000_data = cover_src.read(1)\n",
    "        print(\"  Loaded Tree Cover 2000 data\")\n",
    "\n",
    "    # --- Create mask for non-loss candidate pixels ---\n",
    "    # Condition 1: Sufficient tree cover in 2000\n",
    "    forest_mask = cover_2000_data >= MIN_TREE_COVER\n",
    "    # Condition 2: No loss recorded between 2001 and 2021 (loss year == 0 in Hansen)\n",
    "    no_loss_mask = loss_year_data == 0\n",
    "    # Combined mask for pixels eligible to be sampled as \"non-loss\" controls\n",
    "    non_loss_candidate_mask = forest_mask & no_loss_mask\n",
    "    # Get indices (row, col arrays) where mask is True\n",
    "    non_loss_candidate_indices = np.where(non_loss_candidate_mask)\n",
    "    num_non_loss_candidates = len(non_loss_candidate_indices[0])\n",
    "    print(f\"Found {num_non_loss_candidates} potential non-loss candidate pixels.\")\n",
    "\n",
    "    # Check if we have candidates to sample from\n",
    "    if num_non_loss_candidates == 0:\n",
    "        raise ValueError(\"No non-loss candidate pixels found based on criteria. Cannot sample.\")\n",
    "\n",
    "    # --- Sample pixels year by year ---\n",
    "    np.random.seed(RANDOM_STATE) # for reproducibility\n",
    "\n",
    "    for year_num, target_year in enumerate(YEARS): # YEARS from Cell 5 (e.g., [2001, 2002, 2003])\n",
    "        start_time_year = time.time()\n",
    "        print(f\"\\n--- Sampling for target year: {target_year} ---\")\n",
    "\n",
    "        # 1. Find pixels with loss IN THIS target_year\n",
    "        # Hansen loss year codes 1-21 correspond to years 2001-2021\n",
    "        hansen_loss_code = target_year - 2000\n",
    "        loss_pixels_this_year_mask = loss_year_data == hansen_loss_code\n",
    "        loss_indices_this_year = np.where(loss_pixels_this_year_mask)\n",
    "        num_loss_pixels = len(loss_indices_this_year[0])\n",
    "        print(f\"  Found {num_loss_pixels} loss pixels.\")\n",
    "\n",
    "        if num_loss_pixels == 0:\n",
    "            print(\"  No loss pixels found for this year. Skipping sampling for this year.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Add loss pixels to sample list\n",
    "        for r, c in zip(loss_indices_this_year[0], loss_indices_this_year[1]):\n",
    "            all_sampled_points.append({'row': r, 'col': c, 'target_year': target_year, 'loss': 1})\n",
    "\n",
    "        # 3. Sample non-loss pixels\n",
    "        # Calculate how many non-loss points to sample for this year\n",
    "        num_non_loss_to_sample = min(num_non_loss_candidates, num_loss_pixels * NON_LOSS_RATIO)\n",
    "        print(f\"  Sampling {num_non_loss_to_sample} non-loss pixels (NON_LOSS_RATIO={NON_LOSS_RATIO}).\")\n",
    "\n",
    "        if num_non_loss_to_sample > 0:\n",
    "            # Randomly choose indices from the non_loss_candidate_indices array\n",
    "            sampled_candidate_indices_idx = np.random.choice(\n",
    "                num_non_loss_candidates, num_non_loss_to_sample, replace=False # No replacement\n",
    "            )\n",
    "            # Get the actual row and column values using these sampled indices\n",
    "            sampled_non_loss_rows = non_loss_candidate_indices[0][sampled_candidate_indices_idx]\n",
    "            sampled_non_loss_cols = non_loss_candidate_indices[1][sampled_candidate_indices_idx]\n",
    "\n",
    "            # Add non-loss pixels to sample list\n",
    "            for r, c in zip(sampled_non_loss_rows, sampled_non_loss_cols):\n",
    "                all_sampled_points.append({'row': r, 'col': c, 'target_year': target_year, 'loss': 0})\n",
    "        \n",
    "        current_total_points = len(all_sampled_points)\n",
    "        points_this_year = num_loss_pixels + num_non_loss_to_sample\n",
    "        print(f\"  Finished sampling for {target_year}. Points this year: {points_this_year}. Cumulative: {current_total_points}. Time: {time.time() - start_time_year:.2f}s\")\n",
    "\n",
    "    # --- Cleanup large arrays from memory ---\n",
    "    del loss_year_data, cover_2000_data, forest_mask, no_loss_mask, non_loss_candidate_mask\n",
    "    del loss_pixels_this_year_mask, loss_indices_this_year # non_loss_candidate_indices is large too\n",
    "    if 'non_loss_candidate_indices' in locals(): del non_loss_candidate_indices\n",
    "    import gc\n",
    "    gc.collect() # Try to free memory\n",
    "\n",
    "    # Shuffle all collected points once at the end\n",
    "    if all_sampled_points: # Only shuffle if list is not empty\n",
    "        all_sampled_points = shuffle(all_sampled_points, random_state=RANDOM_STATE)\n",
    "        print(f\"\\nTotal points sampled across all years (before any subsampling): {len(all_sampled_points)}\")\n",
    "        # Optional: Quick check on class balance\n",
    "        loss_counts = pd.Series([p['loss'] for p in all_sampled_points]).value_counts()\n",
    "        print(f\"Sampled loss counts (before any subsampling):\\n{loss_counts}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: No points were sampled across any year. Check data or sampling logic.\")\n",
    "\n",
    "\n",
    "    print(f\"Sampling finished. Total time: {time.time() - sampling_start_time:.2f}s\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     print(f\"ERROR: Required file not found during sampling: {e}. Please ensure data exists and paths in Cell 5 are correct.\")\n",
    "except ValueError as e:\n",
    "     print(f\"ERROR during sampling setup: {e}\")\n",
    "except NameError as e:\n",
    "     print(f\"ERROR: A variable was not defined (likely from Cell 5 or Cell 8). {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during sampling: {type(e).__name__} - {e}\")\n",
    "    # raise e # Uncomment to see full traceback if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "555c1ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original number of sampled points from Cell 10: 3615158\n",
      "Aggressively subsampling to approximately 100000 points for speed...\n",
      "New number of points to process for features: 100000\n",
      "Subsampled loss counts (approximate due to random slice):\n",
      "0    50037\n",
      "1    49963\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a much smaller maximum number of samples to actually use for feature extraction\n",
    "MAX_SAMPLES_FOR_SPRINT = 100000  # Aim for 100k total samples for feature extraction\n",
    "                                 # This is a huge reduction if all_sampled_points is large.\n",
    "\n",
    "# Check if all_sampled_points exists from Cell 10 and has data\n",
    "if 'all_sampled_points' in locals() and all_sampled_points: # Check if list is not empty\n",
    "    original_sample_count = len(all_sampled_points)\n",
    "    print(f\"\\nOriginal number of sampled points from Cell 10: {original_sample_count}\")\n",
    "\n",
    "    if original_sample_count > MAX_SAMPLES_FOR_SPRINT:\n",
    "        print(f\"Aggressively subsampling to approximately {MAX_SAMPLES_FOR_SPRINT} points for speed...\")\n",
    "        \n",
    "        # Ensure shuffling before taking a slice for randomness if not already shuffled,\n",
    "        # or re-shuffle if you want extra randomness for the slice.\n",
    "        # Cell 10 already shuffles at the end, so this might be redundant but harmless.\n",
    "        all_sampled_points = shuffle(all_sampled_points, random_state=RANDOM_STATE) # RANDOM_STATE from Cell 5\n",
    "        \n",
    "        # Take the slice\n",
    "        all_sampled_points = all_sampled_points[:MAX_SAMPLES_FOR_SPRINT]\n",
    "        print(f\"New number of points to process for features: {len(all_sampled_points)}\")\n",
    "\n",
    "        # Optional: Check class balance of the subsample\n",
    "        if all_sampled_points: # Check if list is not empty after slicing\n",
    "            subsample_loss_counts = pd.Series([p['loss'] for p in all_sampled_points]).value_counts()\n",
    "            print(f\"Subsampled loss counts (approximate due to random slice):\\n{subsample_loss_counts}\")\n",
    "        else:\n",
    "            print(\"WARNING: Subsampling resulted in an empty list of points.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Number of sampled points ({original_sample_count}) is already within or below MAX_SAMPLES_FOR_SPRINT ({MAX_SAMPLES_FOR_SPRINT}). No further subsampling applied.\")\n",
    "else:\n",
    "    print(\"WARNING: 'all_sampled_points' not found or is empty. Ensure Cell 10 (Sampling) ran successfully and produced points.\")\n",
    "\n",
    "# --- End of subsampling ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af34867",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "Iterate through the `all_sampled_points`. For each point (row, col, target_year, loss):\n",
    "- Read pixel values from Landsat(target_year - 1).\n",
    "- Read pixel values from Climate(target_year - 1).\n",
    "- Read pixel values from Climate(target_year).\n",
    "- Read pixel values from DEM and Slope.\n",
    "- Store features and target in a structure suitable for Scikit-learn (e.g., NumPy array or Pandas DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589654fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature extraction (NaN DEBUG MODE - dropna disabled)...\n",
      "Attempting to extract features for 100000 points...\n"
     ]
    }
   ],
   "source": [
    "# In Cell 12 of your notebook (Feature Extraction)\n",
    "\n",
    "# Ensure necessary libraries are available\n",
    "# import pandas as pd\n",
    "# import rasterio\n",
    "# from rasterio.windows import Window\n",
    "# from pathlib import Path\n",
    "# import time\n",
    "# import gc\n",
    "\n",
    "# Prepare for feature extraction\n",
    "features = [] # Initialize or clear if re-running\n",
    "# Target 'y' is implicitly defined by the 'loss' key in all_sampled_points\n",
    "# The all_sampled_points variable will now be the SUBSAMPLED list if Cell 10.1 ran.\n",
    "\n",
    "# These should be defined from Cell 5 or Cell 8:\n",
    "# LANDSAT_DIR, CLIMATE_DIR, AUX_DIR, DEM_PATH, SLOPE_PATH\n",
    "# LANDSAT_BANDS, RANDOM_STATE (though RANDOM_STATE not directly used here)\n",
    "\n",
    "print(\"Starting feature extraction (NaN DEBUG MODE - dropna disabled)...\") # Indicate debug mode\n",
    "extraction_start_time = time.time()\n",
    "\n",
    "# Cache for opened rasterio file handles\n",
    "open_files_cache = {}\n",
    "\n",
    "def get_file_handle(path_obj, cache): # path_obj is a Path object\n",
    "    \"\"\"Gets or opens a rasterio file handle, caching it.\"\"\"\n",
    "    path_str = str(path_obj) # Use string representation as key for cache\n",
    "    if path_str not in cache:\n",
    "        if not path_obj.exists():\n",
    "             raise FileNotFoundError(f\"Required file for feature extraction not found: {path_str}\")\n",
    "        # print(f\"  Opening file: {path_obj.name}\") # Can be verbose, uncomment if debugging\n",
    "        cache[path_str] = rasterio.open(path_str)\n",
    "    return cache[path_str]\n",
    "\n",
    "def get_pixel_value_at_rc(src, row, col):\n",
    "    \"\"\"Reads pixel value(s) for all bands at a specific row, col.\"\"\"\n",
    "    window = Window(col, row, 1, 1)\n",
    "    return src.read(window=window).squeeze() # squeeze removes singleton dimensions\n",
    "\n",
    "# --- Process all sampled points (which should be the subsampled list now) ---\n",
    "extraction_errors = 0\n",
    "\n",
    "# Check if all_sampled_points exists and is iterable\n",
    "if 'all_sampled_points' not in locals() or not isinstance(all_sampled_points, list):\n",
    "    raise NameError(\"'all_sampled_points' is not defined or not a list. Run Cell 10 and Cell 10.1 first.\")\n",
    "if not all_sampled_points: # Check if the list is empty\n",
    "    print(\"WARNING: 'all_sampled_points' is empty. No features to extract. Did subsampling remove all points?\")\n",
    "\n",
    "num_points_to_process = len(all_sampled_points)\n",
    "print(f\"Attempting to extract features for {num_points_to_process} points...\")\n",
    "\n",
    "for i, point in enumerate(all_sampled_points):\n",
    "    row, col = point['row'], point['col']\n",
    "    t_year = point['target_year'] # Year loss status is defined for\n",
    "    t_minus_1 = t_year - 1      # Year for predictor data\n",
    "\n",
    "    # Print progress periodically (e.g., every 10% or fixed number)\n",
    "    if num_points_to_process > 0 and (i + 1) % max(1, num_points_to_process // 10) == 0 :\n",
    "         elapsed_time = time.time() - extraction_start_time\n",
    "         points_per_sec = (i + 1) / elapsed_time if elapsed_time > 0 else 0\n",
    "         print(f\"  Processed {(i + 1)}/{num_points_to_process} points... ({points_per_sec:.1f} points/sec)\")\n",
    "\n",
    "    # --- Feature dictionary for this point ---\n",
    "    point_features = {'row': row, 'col': col, 'target_year': t_year, 'loss': point['loss']}\n",
    "\n",
    "    try:\n",
    "        # --- Landsat (t-1) ---\n",
    "        lsat_path = LANDSAT_DIR / f'landsat_composite_{t_minus_1}.tif'\n",
    "        lsat_src = get_file_handle(lsat_path, open_files_cache)\n",
    "        lsat_values = get_pixel_value_at_rc(lsat_src, row, col)\n",
    "        if not isinstance(lsat_values, np.ndarray) or lsat_values.ndim == 0:\n",
    "            if len(LANDSAT_BANDS) == 1:\n",
    "                lsat_values_array = np.array([lsat_values]) if not isinstance(lsat_values, np.ndarray) else lsat_values.reshape(1)\n",
    "            else:\n",
    "                # If we expect multiple bands but got a scalar, fill with NaNs for all bands\n",
    "                # print(f\"DEBUG: Got scalar/0-dim Landsat for point {i}, year {t_minus_1}. Filling with NaNs.\")\n",
    "                lsat_values_array = np.full(len(LANDSAT_BANDS), np.nan)\n",
    "        else:\n",
    "            lsat_values_array = lsat_values\n",
    "\n",
    "        if len(lsat_values_array) != len(LANDSAT_BANDS):\n",
    "            # This case should ideally be caught by the GEE export or raster verification if band counts differ.\n",
    "            # If it happens here, it means pixel read is inconsistent. Fill with NaNs.\n",
    "            # print(f\"DEBUG: Band count mismatch for point {i}, year {t_minus_1}. Read {len(lsat_values_array)}, expected {len(LANDSAT_BANDS)}. Filling with NaNs.\")\n",
    "            lsat_values_array = np.full(len(LANDSAT_BANDS), np.nan)\n",
    "            \n",
    "        for band_name, band_value in zip(LANDSAT_BANDS, lsat_values_array):\n",
    "            point_features[f'lsat_{band_name}_{t_minus_1}'] = band_value\n",
    "\n",
    "\n",
    "        # --- Climate (t-1) ---\n",
    "        temp_tm1_path = CLIMATE_DIR / f'mean_temp_{t_minus_1}.tif'\n",
    "        precip_tm1_path = CLIMATE_DIR / f'total_precip_{t_minus_1}.tif'\n",
    "        temp_tm1_src = get_file_handle(temp_tm1_path, open_files_cache)\n",
    "        precip_tm1_src = get_file_handle(precip_tm1_path, open_files_cache)\n",
    "        point_features[f'temp_{t_minus_1}'] = get_pixel_value_at_rc(temp_tm1_src, row, col)\n",
    "        point_features[f'precip_{t_minus_1}'] = get_pixel_value_at_rc(precip_tm1_src, row, col)\n",
    "\n",
    "        # --- Climate (t) ---\n",
    "        temp_t_path = CLIMATE_DIR / f'mean_temp_{t_year}.tif'\n",
    "        precip_t_path = CLIMATE_DIR / f'total_precip_{t_year}.tif'\n",
    "        temp_t_src = get_file_handle(temp_t_path, open_files_cache)\n",
    "        precip_t_src = get_file_handle(precip_t_path, open_files_cache)\n",
    "        point_features[f'temp_{t_year}'] = get_pixel_value_at_rc(temp_t_src, row, col)\n",
    "        point_features[f'precip_{t_year}'] = get_pixel_value_at_rc(precip_t_src, row, col)\n",
    "\n",
    "        # --- Static Vars (Only if 'dem' and 'slope' are in STATIC_VARS from Cell 5) ---\n",
    "        if 'dem' in STATIC_VARS: # STATIC_VARS defined in Cell 5\n",
    "            dem_src = get_file_handle(DEM_PATH, open_files_cache)\n",
    "            point_features['dem'] = get_pixel_value_at_rc(dem_src, row, col)\n",
    "        if 'slope' in STATIC_VARS: # STATIC_VARS defined in Cell 5\n",
    "            slope_src = get_file_handle(SLOPE_PATH, open_files_cache)\n",
    "            point_features['slope'] = get_pixel_value_at_rc(slope_src, row, col)\n",
    "\n",
    "        # --- Append the complete feature dictionary ---\n",
    "        features.append(point_features)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "         # This error should have been caught by get_file_handle but handle defensively\n",
    "         # print(f\"WARNING: Skipping point {i} due to missing file during feature extraction: {e}\") # Can be very verbose\n",
    "         extraction_errors += 1\n",
    "         # Fill with NaNs if a file is missing to keep row structure, NaNs will be counted later\n",
    "         # Create a placeholder with NaNs for expected features if we can't read\n",
    "         for band_name in LANDSAT_BANDS: point_features[f'lsat_{band_name}_{t_minus_1}'] = np.nan\n",
    "         point_features[f'temp_{t_minus_1}'] = np.nan\n",
    "         point_features[f'precip_{t_minus_1}'] = np.nan\n",
    "         point_features[f'temp_{t_year}'] = np.nan\n",
    "         point_features[f'precip_{t_year}'] = np.nan\n",
    "         if 'dem' in STATIC_VARS: point_features['dem'] = np.nan\n",
    "         if 'slope' in STATIC_VARS: point_features['slope'] = np.nan\n",
    "         features.append(point_features) # Append the dict with NaNs\n",
    "         continue\n",
    "    except Exception as e:\n",
    "        # print(f\"ERROR extracting features for point {i} (row={row}, col={col}, target_year={t_year}): {type(e).__name__} - {e}\") # Can be very verbose\n",
    "        extraction_errors += 1\n",
    "        # Fill with NaNs for this problematic point\n",
    "        for band_name in LANDSAT_BANDS: point_features[f'lsat_{band_name}_{t_minus_1}'] = np.nan\n",
    "        point_features[f'temp_{t_minus_1}'] = np.nan\n",
    "        point_features[f'precip_{t_minus_1}'] = np.nan\n",
    "        point_features[f'temp_{t_year}'] = np.nan\n",
    "        point_features[f'precip_{t_year}'] = np.nan\n",
    "        if 'dem' in STATIC_VARS: point_features['dem'] = np.nan\n",
    "        if 'slope' in STATIC_VARS: point_features['slope'] = np.nan\n",
    "        features.append(point_features) # Append the dict with NaNs\n",
    "        continue\n",
    "\n",
    "# --- Close all opened files in the cache ---\n",
    "print(\"\\nClosing cached raster files...\")\n",
    "for path_str_cached, src_cached in open_files_cache.items(): # Use different var names\n",
    "    try:\n",
    "        src_cached.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing file {path_str_cached}: {e}\")\n",
    "open_files_cache.clear() # Clear the cache explicitly\n",
    "\n",
    "# --- Final summary of feature extraction ---\n",
    "final_elapsed_time = time.time() - extraction_start_time\n",
    "print(f\"Finished feature extraction. Total time: {final_elapsed_time:.2f}s\")\n",
    "print(f\"Attempted to process {num_points_to_process} points. Appended {len(features)} feature sets (should be same).\")\n",
    "if extraction_errors > 0:\n",
    "    print(f\"Encountered {extraction_errors} errors during individual pixel reads (features for these points were set to NaN).\")\n",
    "\n",
    "# --- Convert list of dictionaries to DataFrame ---\n",
    "if not features:\n",
    "    print(\"CRITICAL WARNING: No features were constructed (features list is empty). Cannot proceed to modeling.\")\n",
    "    # raise SystemExit(\"No features constructed. Halting.\") # Keep this commented for now\n",
    "else:\n",
    "    print(\"\\nConverting extracted features to DataFrame...\")\n",
    "    feature_df = pd.DataFrame(features)\n",
    "    del features # Free up memory from the list of dicts\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"DataFrame shape (before any dropna): {feature_df.shape}\")\n",
    "    print(\"Sample data (first 5 rows, may contain NaNs):\")\n",
    "    print(feature_df.head())\n",
    "    \n",
    "    print(\"\\nFull check for missing values (NaNs) per column:\")\n",
    "    nan_counts = feature_df.isnull().sum()\n",
    "    # Print all columns and their NaN counts, even if zero, for full visibility\n",
    "    print(nan_counts)\n",
    "    total_nans = nan_counts.sum()\n",
    "    if total_nans == 0:\n",
    "        print(\"SUCCESS: No NaN values found in the entire feature DataFrame!\")\n",
    "    else:\n",
    "        print(f\"WARNING: Found a total of {total_nans} NaN values across all columns.\")\n",
    "        print(\"Columns with NaN counts > 0:\")\n",
    "        print(nan_counts[nan_counts > 0])\n",
    "\n",
    "\n",
    "    # --- MODIFICATION: Handle Missing Data (Temporarily Disabled dropna) ---\n",
    "    print(\"\\nSKIPPING dropna() for NaN DEBUGGING.\")\n",
    "    # feature_df = feature_df.dropna() # <--- THIS IS COMMENTED OUT FOR DEBUGGING\n",
    "    initial_rows = len(feature_df) # Will be same as final_rows for now\n",
    "    final_rows = len(feature_df)\n",
    "\n",
    "    # if initial_rows != final_rows: # This condition will be false now\n",
    "    #     print(f\"\\nDropped {initial_rows - final_rows} rows containing NaN values.\")\n",
    "    #     print(f\"Final DataFrame shape after dropna: {feature_df.shape}\")\n",
    "    # else:\n",
    "    # print(\"\\nNo rows dropped due to NaN values (because dropna is disabled).\")\n",
    "\n",
    "    if final_rows == 0 and initial_rows > 0 :\n",
    "        # This condition should not be met if dropna() is disabled and features list was not empty\n",
    "        print(\"DEBUG WARNING: DataFrame is empty but dropna was disabled. Problem likely in feature list creation.\")\n",
    "    elif final_rows == 0 and initial_rows == 0:\n",
    "         print(\"CRITICAL WARNING: Feature DataFrame is empty (likely no points processed or all had errors).\")\n",
    "         # raise SystemExit(\"Feature DataFrame empty. Halting.\") # Keep commented for now\n",
    "    else:\n",
    "        # --- Prepare final X and y for modeling (will contain NaNs if present) ---\n",
    "        target_series = feature_df['loss']\n",
    "        year_series = feature_df['target_year'] # Keep track of year for temporal split\n",
    "\n",
    "        columns_to_drop_for_X = ['row', 'col', 'target_year', 'loss']\n",
    "        columns_present_to_drop = [col for col in columns_to_drop_for_X if col in feature_df.columns]\n",
    "        \n",
    "        X = feature_df.drop(columns=columns_present_to_drop)\n",
    "        y = target_series\n",
    "\n",
    "        FEATURE_NAMES = X.columns.tolist()\n",
    "\n",
    "        print(f\"\\nShape of X (features, may contain NaNs): {X.shape}\")\n",
    "        print(f\"Shape of y (target): {y.shape}\")\n",
    "        print(\"\\nFinal feature columns used for X (model input):\", FEATURE_NAMES)\n",
    "        print(\"\\nTarget distribution (y):\") # Note: y should not have NaNs as it comes from 'loss'\n",
    "        print(y.value_counts(normalize=True, dropna=False)) # dropna=False to see if y itself has NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf682b1",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Train/Test Split (Temporal): TODO\n",
    "\n",
    "Split the data based on the `target_year`. Data from years before `TEST_SPLIT_YEAR` will be used for training, and data from `TEST_SPLIT_YEAR` onwards for testing. This prevents data leakage from the future into the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Splitting data temporally based on year {TEST_SPLIT_YEAR}...\")\n",
    "\n",
    "# Use the 'year_series' created alongside X and y\n",
    "train_mask = year_series < TEST_SPLIT_YEAR\n",
    "test_mask = year_series >= TEST_SPLIT_YEAR\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "# Keep track of years for verification (optional)\n",
    "# train_years = year_series[train_mask]\n",
    "# test_years = year_series[test_mask]\n",
    "\n",
    "# Cleanup intermediate objects if memory is tight\n",
    "# del X, y, feature_df, year_series, train_mask, test_mask\n",
    "# gc.collect()\n",
    "\n",
    "print(f\"Train set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Verify the split worked as expected\n",
    "if not X_train.empty and not X_test.empty:\n",
    "    print(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"Test target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "    # print(f\"Train years range: {train_years.min()} - {train_years.max()}\")\n",
    "    # print(f\"Test years range: {test_years.min()} - {test_years.max()}\")\n",
    "else:\n",
    "    print(\"WARNING: Train or Test set is empty after split. Check TEST_SPLIT_YEAR and data distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8883b8",
   "metadata": {},
   "source": [
    "## 7. Model Training (Random Forest)\n",
    "\n",
    "Train a Random Forest Classifier on the training data. We use `class_weight='balanced'` to help handle potential class imbalance between loss and no-loss pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data exists\n",
    "if X_train.empty or y_train.empty:\n",
    "    raise SystemExit(\"Training data is empty. Cannot train model.\")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "train_start_time = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=RF_N_ESTIMATORS,\n",
    "    max_depth=RF_MAX_DEPTH,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=RF_N_JOBS,             # Use all available cores\n",
    "    class_weight='balanced',      # Adjust for class imbalance\n",
    "    oob_score=False               # Set to True to estimate generalization score without test set (slower)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Finished training. Total time: {time.time() - train_start_time:.2f}s\")\n",
    "\n",
    "# Optional: Print OOB score if calculated\n",
    "# if rf_classifier.oob_score:\n",
    "#     print(f\"Out-of-Bag (OOB) Score: {rf_classifier.oob_score_:.4f}\")\n",
    "\n",
    "# --- Save the trained model ---\n",
    "model_filename = OUTPUT_DIR / f'rf_forest_loss_model_{START_YEAR}_{END_YEAR}.joblib'\n",
    "try:\n",
    "    joblib.dump(rf_classifier, model_filename)\n",
    "    print(f\"Model saved successfully to: {model_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688921c",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Evaluate the trained model's performance on the temporally independent test set using various classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test data exists\n",
    "if X_test.empty or y_test.empty:\n",
    "    print(\"Test data is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    print(\"Evaluating model on the test set...\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1] # Probability of class 1 (loss)\n",
    "\n",
    "    # --- Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Use zero_division=0 to avoid warnings if a class has no predicted samples\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Loss', 'Loss'], zero_division=0))\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    try:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Loss', 'Loss'])\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate Confusion Matrix plot: {e}\")\n",
    "\n",
    "\n",
    "    # --- ROC AUC Score ---\n",
    "    # Check if both classes are present in y_test for ROC AUC calculation\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "        # Optional: Plot ROC Curve\n",
    "        try:\n",
    "            from sklearn.metrics import RocCurveDisplay\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            RocCurveDisplay.from_predictions(y_test, y_pred_proba, ax=ax, name='Random Forest')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate ROC Curve plot: {e}\")\n",
    "    else:\n",
    "        print(\"\\nROC AUC Score cannot be calculated: Only one class present in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e6418",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Analyze the feature importances provided by the Random Forest model (based on mean decrease in impurity - Gini importance) to understand which variables were most influential in the model's predictions.g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating and plotting Feature Importances...\")\n",
    "\n",
    "try:\n",
    "    importances = rf_classifier.feature_importances_\n",
    "    # Get standard deviations of importances across trees (optional, adds compute time)\n",
    "    # std = np.std([tree.feature_importances_ for tree in rf_classifier.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1] # Sort features by importance (descending)\n",
    "\n",
    "    # --- Create DataFrame for easier plotting/viewing ---\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': [FEATURE_NAMES[i] for i in indices],\n",
    "        'Importance': importances[indices],\n",
    "        # 'StdDev': std[indices] # Uncomment if std is calculated\n",
    "    })\n",
    "\n",
    "    print(\"\\nTop 20 Feature Importances:\")\n",
    "    print(importance_df.head(20))\n",
    "\n",
    "    # --- Plot Feature Importances ---\n",
    "    N_FEATURES_TO_PLOT = 20\n",
    "    plt.figure(figsize=(12, max(6, N_FEATURES_TO_PLOT // 2))) # Adjust height based on number of features\n",
    "    plt.title(f\"Feature Importances (Top {N_FEATURES_TO_PLOT})\")\n",
    "    plt.barh(range(N_FEATURES_TO_PLOT), # Use barh for horizontal plot\n",
    "             importance_df['Importance'].head(N_FEATURES_TO_PLOT)[::-1], # Plot descending importance\n",
    "             # xerr=importance_df['StdDev'].head(N_FEATURES_TO_PLOT)[::-1], # Uncomment if std is calculated\n",
    "             align='center')\n",
    "    plt.yticks(range(N_FEATURES_TO_PLOT),\n",
    "               importance_df['Feature'].head(N_FEATURES_TO_PLOT)[::-1]) # Labels for y-axis\n",
    "    plt.xlabel(\"Mean Decrease in Impurity (Gini Importance)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim([-1, N_FEATURES_TO_PLOT])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Save importance data\n",
    "    importance_filename = OUTPUT_DIR / f'rf_feature_importances_{START_YEAR}_{END_YEAR}.csv'\n",
    "    importance_df.to_csv(importance_filename, index=False)\n",
    "    print(f\"Feature importances saved to: {importance_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during feature importance calculation or plotting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de410a1d",
   "metadata": {},
   "source": [
    "## 10. (Optional) Prediction Map Visualization\n",
    "\n",
    "This section provides a function to generate a spatial map of predicted loss probability for a given year using the trained model. This is computationally intensive and requires significant memory/time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9144a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This section is computationally intensive ---\n",
    "\n",
    "def predict_loss_probability_map(model, feature_names_ordered, output_path, year_to_predict):\n",
    "    \"\"\"Generates a GeoTIFF map of predicted loss probability for a given year.\"\"\"\n",
    "    print(f\"\\nGenerating loss probability map for year: {year_to_predict}\")\n",
    "    prediction_start_time = time.time()\n",
    "    t_minus_1 = year_to_predict - 1\n",
    "\n",
    "    # --- Define input file paths for the prediction year ---\n",
    "    lsat_path = LANDSAT_DIR / f'landsat_composite_{t_minus_1}.tif'\n",
    "    temp_tm1_path = CLIMATE_DIR / f'mean_temp_{t_minus_1}.tif'\n",
    "    precip_tm1_path = CLIMATE_DIR / f'total_precip_{t_minus_1}.tif'\n",
    "    temp_t_path = CLIMATE_DIR / f'mean_temp_{year_to_predict}.tif'\n",
    "    precip_t_path = CLIMATE_DIR / f'total_precip_{year_to_predict}.tif'\n",
    "    dem_path_pred = DEM_PATH # Static path\n",
    "    slope_path_pred = SLOPE_PATH # Static path\n",
    "\n",
    "    # Check if all required files exist before starting\n",
    "    required_files = {\n",
    "        'lsat': lsat_path, 'temp_tm1': temp_tm1_path, 'prec_tm1': precip_tm1_path,\n",
    "        'temp_t': temp_t_path, 'prec_t': precip_t_path, 'dem': dem_path_pred, 'slope': slope_path_pred\n",
    "    }\n",
    "    missing_files = [name for name, p in required_files.items() if not p.exists()]\n",
    "    if missing_files:\n",
    "        print(f\"ERROR: Missing required raster files for prediction year {year_to_predict}: {missing_files}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Open source files and get metadata from a reference (e.g., Landsat)\n",
    "        handles = {name: rasterio.open(p) for name, p in required_files.items()}\n",
    "        ref_src = handles['lsat']\n",
    "        profile = ref_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1, nodata=-9999.0) # Output is probability (float)\n",
    "\n",
    "        # Create the output file\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            # Define processing blocks (adjust block size based on available RAM)\n",
    "            block_shape = (512, 512) # Larger block might be faster if RAM allows\n",
    "            total_blocks = len(list(dst.block_windows(block_shape)))\n",
    "            processed_blocks = 0\n",
    "\n",
    "            print(f\"Processing {total_blocks} blocks...\")\n",
    "            for block_info, window in dst.block_windows(block_shape):\n",
    "                processed_blocks += 1\n",
    "                if processed_blocks % max(1, total_blocks // 10) == 0: # Print progress roughly 10 times\n",
    "                    print(f\"  Processing block {processed_blocks}/{total_blocks}...\")\n",
    "\n",
    "                # Read data for the block from all sources\n",
    "                block_data = {}\n",
    "                # Read Landsat (all bands)\n",
    "                block_data['lsat'] = handles['lsat'].read(window=window)\n",
    "                # Read single band rasters\n",
    "                for name in ['temp_tm1', 'prec_tm1', 'temp_t', 'prec_t', 'dem', 'slope']:\n",
    "                    block_data[name] = handles[name].read(1, window=window)\n",
    "\n",
    "                block_h, block_w = block_data['lsat'].shape[1], block_data['lsat'].shape[2]\n",
    "                if block_h == 0 or block_w == 0: continue # Skip empty blocks\n",
    "                n_pixels_in_block = block_h * block_w\n",
    "\n",
    "                # --- Assemble features for the block ---\n",
    "                # Important: Feature order must match training order (FEATURE_NAMES)\n",
    "                block_features_list = []\n",
    "                for feature_name in feature_names_ordered:\n",
    "                    if feature_name.startswith('lsat_'):\n",
    "                        # Extract band name and check year (should be t-1)\n",
    "                        parts = feature_name.split('_')\n",
    "                        band_name = parts[1]\n",
    "                        try:\n",
    "                            band_index = LANDSAT_BANDS.index(band_name)\n",
    "                            feature_data = block_data['lsat'][band_index].ravel()\n",
    "                        except ValueError:\n",
    "                            raise ValueError(f\"Unknown Landsat band '{band_name}' in feature name list.\")\n",
    "                    elif feature_name.startswith('temp_'):\n",
    "                        year_suffix = feature_name.split('_')[1]\n",
    "                        if year_suffix == str(t_minus_1):\n",
    "                            feature_data = block_data['temp_tm1'].ravel()\n",
    "                        elif year_suffix == str(year_to_predict):\n",
    "                            feature_data = block_data['temp_t'].ravel()\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unexpected year suffix in temp feature: {feature_name}\")\n",
    "                    elif feature_name.startswith('precip_'):\n",
    "                        year_suffix = feature_name.split('_')[1]\n",
    "                        if year_suffix == str(t_minus_1):\n",
    "                            feature_data = block_data['prec_tm1'].ravel()\n",
    "                        elif year_suffix == str(year_to_predict):\n",
    "                            feature_data = block_data['prec_t'].ravel()\n",
    "                        else:\n",
    "                             raise ValueError(f\"Unexpected year suffix in precip feature: {feature_name}\")\n",
    "                    elif feature_name == 'dem':\n",
    "                        feature_data = block_data['dem'].ravel()\n",
    "                    elif feature_name == 'slope':\n",
    "                        feature_data = block_data['slope'].ravel()\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown feature name structure: {feature_name}\")\n",
    "                    block_features_list.append(feature_data)\n",
    "\n",
    "                # Combine into (n_pixels, n_features) array\n",
    "                block_features_array = np.vstack(block_features_list).T\n",
    "\n",
    "                # --- Handle potential NoData/NaN values ---\n",
    "                # Check for NaNs across all features for each pixel\n",
    "                valid_pixel_mask = ~np.isnan(block_features_array).any(axis=1)\n",
    "                # Create output block array, fill with NoData initially\n",
    "                out_block = np.full(n_pixels_in_block, profile['nodata'], dtype=np.float32)\n",
    "\n",
    "                # Predict only for valid pixels\n",
    "                if np.any(valid_pixel_mask):\n",
    "                    valid_features = block_features_array[valid_pixel_mask, :]\n",
    "                    # Predict probability of class 1 (loss)\n",
    "                    pred_proba = model.predict_proba(valid_features)[:, 1]\n",
    "                    # Fill predictions into the output block based on the valid mask\n",
    "                    out_block[valid_pixel_mask] = pred_proba\n",
    "\n",
    "                # Reshape back to 2D block and write\n",
    "                out_block = out_block.reshape(block_h, block_w)\n",
    "                dst.write(out_block.astype(rasterio.float32), 1, window=window)\n",
    "\n",
    "        # Close all source file handles\n",
    "        for handle in handles.values():\n",
    "            handle.close()\n",
    "\n",
    "        print(f\"Probability map saved to: {output_path}\")\n",
    "        print(f\"Prediction map generation time: {time.time() - prediction_start_time:.2f}s\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR generating prediction map: Input file not found - {e}\")\n",
    "    except MemoryError:\n",
    "        print(\"ERROR generating prediction map: Insufficient memory. Try smaller block_shape.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR generating prediction map: {type(e).__name__} - {e}\")\n",
    "\n",
    "\n",
    "# --- Example Call (run only if needed and model is trained) ---\n",
    "# Check if model and feature names exist before running\n",
    "# if 'rf_classifier' in locals() and 'FEATURE_NAMES' in locals() and Path(model_filename).exists():\n",
    "#     PREDICTION_YEAR = 2021 # Choose a year from the test set range\n",
    "#     prob_map_filename = OUTPUT_DIR / f'rf_loss_probability_{PREDICTION_YEAR}.tif'\n",
    "#     # Optional: Load model if kernel restarted\n",
    "#     # print(f\"Loading model from {model_filename}...\")\n",
    "#     # loaded_model = joblib.load(model_filename)\n",
    "#     # predict_loss_probability_map(loaded_model, FEATURE_NAMES, prob_map_filename, PREDICTION_YEAR)\n",
    "#     # OR use the model in memory if kernel wasn't restarted:\n",
    "#     predict_loss_probability_map(rf_classifier, FEATURE_NAMES, prob_map_filename, PREDICTION_YEAR)\n",
    "# else:\n",
    "#     print(\"\\nModel or feature names not available. Skipping prediction map generation.\")\n",
    "#     if not 'rf_classifier' in locals(): print(\" Reason: 'rf_classifier' not found.\")\n",
    "#     if not 'FEATURE_NAMES' in locals(): print(\" Reason: 'FEATURE_NAMES' not found.\")\n",
    "#     if not Path(model_filename).exists(): print(f\" Reason: Model file not found at {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7e88a",
   "metadata": {},
   "source": [
    "## 11. Conclusion & Next Steps\n",
    "\n",
    "- Summarize findings from evaluation metrics (e.g., Accuracy, Precision, Recall, F1, ROC AUC) and the feature importance analysis. Which factors were most predictive of forest loss in this region according to the model?\n",
    "- Discuss limitations:\n",
    "    - **Correlation vs Causation:** The model identifies correlations, not necessarily causal links.\n",
    "    - **Data Resolution:** 30m resolution might miss fine-scale drivers. Climate data resolution (TerraClimate ~4km) is much coarser than Landsat/Hansen.\n",
    "    - **Loss Type:** Hansen data aggregates loss from various causes (logging, pests, wind, etc.). The model doesn't differentiate these.\n",
    "    - **Sampling Bias:** The non-loss sampling strategy might influence results.\n",
    "    - **Spatial Autocorrelation:** This analysis treats pixels independently, ignoring spatial relationships which are likely important in forest dynamics.\n",
    "    - **Model Simplicity:** Random Forest is robust but might not capture complex spatio-temporal interactions as well as more advanced models (if data/time permitted).\n",
    "- Suggest potential future work:\n",
    "    - Hyperparameter tuning of the Random Forest model (e.g., using GridSearchCV or RandomizedSearchCV).\n",
    "    - Exploring other models (e.g., Gradient Boosting Machines like XGBoost/LightGBM, potentially simpler deep learning if data is very large).\n",
    "    - Incorporating spatial features (e.g., distance to roads/mills, patch metrics, neighborhood characteristics).\n",
    "    - Using focal statistics on predictor variables to capture neighborhood context.\n",
    "    - Attempting to differentiate loss types if finer-grained data becomes available.\n",
    "    - Analyzing model predictions spatially - where does the model perform well/poorly?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
