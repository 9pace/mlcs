{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61139a47",
   "metadata": {},
   "source": [
    "# Forest Loss Driver Analysis (Northern Maine Acaduab Reguib)\n",
    "\n",
    "**Goal:** Use a Random Forest model to identify the relative importance of\n",
    "preceding spectral conditions and climate variables in predicting forest loss\n",
    "events (from Hansen GFC) between 2001 and 2021.\n",
    "\n",
    "**Data:**\n",
    "- Annual Landsat C02 L2 median composites (2000-2021), Bands: B, G, R, NIR, SWIR1, SWIR2, NDVI, NBR\n",
    "- Hansen GFC Loss Year (2000-2021 v1.9)\n",
    "- Hansen GFC Tree Cover 2000\n",
    "- Annual TerraClimate: Mean Temperature, Total Precipitation (2000-2021)\n",
    "- SRTM DEM & Slope\n",
    "\n",
    "**Methodology:**\n",
    "1. Load and verify data alignment (CRS, Extent, Resolution).\n",
    "2. Define sampling strategy: Sample loss pixels and non-loss pixels (within forest mask) annually.\n",
    "3. Extract features for sampled pixels:\n",
    "   - Target: Loss in year `t` (binary)\n",
    "   - Features: Landsat(t-1), Climate(t-1), Climate(t), DEM, Slope\n",
    "4. Split data temporally (e.g., train 2001-2015, test 2016-2021).\n",
    "5. Train Random Forest Classifier.\n",
    "6. Evaluate model performance.\n",
    "7. Analyze feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938ce21",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4d5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time # Keep track of processing time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.enums import Resampling # if needed for consistency checks\n",
    "# import geopandas as gpd # If needed for vector operations later\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.utils import shuffle # For sampling\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib # To save the trained model\n",
    "\n",
    "# Optional: for parallel processing if sampling is slow\n",
    "# import dask.array as da\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01896d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e84f24a",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/benjaminpace/MLCS/mlcs\n",
      "Data Directory: /Users/benjaminpace/MLCS/mlcs/data\n"
     ]
    }
   ],
   "source": [
    "# --- Project Structure ---\n",
    "# Assuming the notebook is in 'notebooks/', navigate to the project root\n",
    "try:\n",
    "    PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "    # Basic check if 'data' dir exists relative to parent\n",
    "    if not (PROJECT_ROOT / 'data').exists():\n",
    "         # If run directly from project root maybe?\n",
    "         PROJECT_ROOT = Path(os.getcwd())\n",
    "         if not (PROJECT_ROOT / 'data').exists():\n",
    "             raise FileNotFoundError(\"Could not determine project root. Expecting 'data' dir.\")\n",
    "except:\n",
    "    # Fallback if structure is different, manual path needed\n",
    "    PROJECT_ROOT = Path('/path/to/your/project/root') # <--- ADJUST MANUALLY IF NEEDED\n",
    "    print(\"WARNING: Could not auto-detect project root. Set manually.\")\n",
    "    if not (PROJECT_ROOT / 'data').exists():\n",
    "         raise FileNotFoundError(f\"Data directory not found at: {PROJECT_ROOT / 'data'}\")\n",
    "\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "LANDSAT_DIR = DATA_DIR / 'landsat'\n",
    "HANSEN_DIR = DATA_DIR / 'hansen'\n",
    "CLIMATE_DIR = DATA_DIR / 'climate'\n",
    "AUX_DIR = DATA_DIR / 'aux'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output' # Create if you want to save models/plots\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "\n",
    "# --- Data File Paths ---\n",
    "HANSEN_LOSS_YEAR_PATH = HANSEN_DIR / 'hansen_lossyear_2000_2021.tif'\n",
    "HANSEN_COVER_2000_PATH = HANSEN_DIR / 'hansen_treecover_2000.tif'\n",
    "DEM_PATH = AUX_DIR / 'dem.tif'\n",
    "SLOPE_PATH = AUX_DIR / 'slope.tif'\n",
    "\n",
    "# --- Analysis Parameters ---\n",
    "START_YEAR = 2001 # First year for which we can have t-1 predictors (Landsat 2000)\n",
    "END_YEAR = 2021   # Last year of loss data\n",
    "YEARS = list(range(START_YEAR, END_YEAR + 1))\n",
    "\n",
    "# Define the bands we expect in the Landsat composites AFTER GEE processing\n",
    "# IMPORTANT: Verify this order matches your actual GeoTIFF band order!\n",
    "# Check using: `with rasterio.open(example_landsat_path) as src: print(src.descriptions)`\n",
    "LANDSAT_BANDS = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'NDVI', 'NBR']\n",
    "CLIMATE_VARS = ['mean_temp', 'total_precip']\n",
    "STATIC_VARS = ['dem', 'slope'] # From AUX_DIR\n",
    "\n",
    "# --- Sampling Parameters ---\n",
    "# Ratio of non-loss points to sample for every 1 loss point\n",
    "NON_LOSS_RATIO = 2 # Sample 2 non-loss points for every loss point\n",
    "# Minimum tree cover percentage in 2000 to be considered 'forest' for sampling non-loss points\n",
    "MIN_TREE_COVER = 30\n",
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Modeling Parameters ---\n",
    "TEST_SPLIT_YEAR = 2016 # Year to split data temporally\n",
    "RF_N_ESTIMATORS = 150 # Number of trees in the forest (adjust later)\n",
    "RF_MAX_DEPTH = None   # Let trees grow deep initially (adjust later)\n",
    "RF_N_JOBS = -1        # Use all available CPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93418285",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Verification\n",
    "\n",
    "**CRITICAL STEP:** Verify that all input rasters have the *exact same* Coordinate Reference System (CRS), transform (affine), dimensions (width, height), and resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50dd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_raster_alignment(raster_paths):\n",
    "    \"\"\"Checks CRS, transform, shape of multiple rasters.\"\"\"\n",
    "    print(\"Verifying raster alignment...\")\n",
    "    if not raster_paths:\n",
    "        print(\"No raster paths provided.\")\n",
    "        return None\n",
    "\n",
    "    reference_profile = None\n",
    "    is_aligned = True\n",
    "    checked_paths = [] # Keep track of files actually checked\n",
    "\n",
    "    for i, path_obj in enumerate(raster_paths):\n",
    "        path = str(path_obj) # Ensure it's a string for rasterio\n",
    "        if not Path(path).exists():\n",
    "            print(f\"WARNING: File not found, skipping alignment check: {path}\")\n",
    "            # Decide if this should be a fatal error\n",
    "            # is_aligned = False # Uncomment if missing file means failure\n",
    "            continue # Skip to next file\n",
    "\n",
    "        checked_paths.append(path)\n",
    "        try:\n",
    "            with rasterio.open(path) as src:\n",
    "                profile = {\n",
    "                    'path': Path(path).name,\n",
    "                    'crs': src.crs,\n",
    "                    'transform': src.transform,\n",
    "                    'width': src.width,\n",
    "                    'height': src.height,\n",
    "                    'count': src.count, # Number of bands\n",
    "                    'dtype': src.dtype # Data type\n",
    "                }\n",
    "                print(f\"--- Checking: {profile['path']} ---\")\n",
    "                # print(f\"  CRS: {profile['crs']}\")\n",
    "                # print(f\"  Transform: {profile['transform']}\")\n",
    "                # print(f\"  Shape: ({profile['height']}, {profile['width']})\")\n",
    "                # print(f\"  Band Count: {profile['count']}\")\n",
    "                # print(f\"  Data Type: {profile['dtype']}\")\n",
    "\n",
    "\n",
    "                if reference_profile is None:\n",
    "                    reference_profile = profile\n",
    "                    print(f\"  Set as Reference: CRS={profile['crs']}, Shape=({profile['height']},{profile['width']}), Transform={profile['transform']}\")\n",
    "                else:\n",
    "                    # Check alignment\n",
    "                    if profile['crs'] != reference_profile['crs']:\n",
    "                        print(f\"  MISMATCH: CRS {profile['crs']} differs from reference {reference_profile['crs']}\")\n",
    "                        is_aligned = False\n",
    "                    if profile['transform'] != reference_profile['transform']:\n",
    "                        print(f\"  MISMATCH: Transform differs from reference\")\n",
    "                        is_aligned = False\n",
    "                    if profile['width'] != reference_profile['width'] or profile['height'] != reference_profile['height']:\n",
    "                        print(f\"  MISMATCH: Shape ({profile['height']}, {profile['width']}) differs from reference ({reference_profile['height']}, {reference_profile['width']})\")\n",
    "                        is_aligned = False\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR reading {path}: {e}\")\n",
    "            is_aligned = False\n",
    "\n",
    "    if not reference_profile:\n",
    "         print(\"\\nERROR: No valid reference raster found to check alignment against.\")\n",
    "         return None\n",
    "\n",
    "    if is_aligned and checked_paths:\n",
    "        print(f\"\\nSUCCESS: All {len(checked_paths)} checked rasters appear aligned.\")\n",
    "        # Return the common profile derived from the reference\n",
    "        return {k: v for k, v in reference_profile.items() if k != 'path'}\n",
    "    elif not checked_paths:\n",
    "        print(\"\\nWARNING: No files were actually checked (all missing?).\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"\\nERROR: Raster alignment check failed. Please fix data before proceeding.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18180f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gather all files to check ---\n",
    "# Get one example Landsat and Climate file for checking structure\n",
    "# Use year 2000 for t-1 predictors of 2001 loss\n",
    "example_landsat_path = LANDSAT_DIR / f'landsat_composite_2000.tif'\n",
    "example_temp_path = CLIMATE_DIR / f'mean_temp_2000.tif'\n",
    "example_precip_path = CLIMATE_DIR / f'total_precip_2000.tif'\n",
    "\n",
    "files_to_check = [\n",
    "    HANSEN_LOSS_YEAR_PATH,\n",
    "    HANSEN_COVER_2000_PATH,\n",
    "    DEM_PATH,\n",
    "    SLOPE_PATH,\n",
    "    example_landsat_path,\n",
    "    example_temp_path,\n",
    "    example_precip_path,\n",
    "]\n",
    "\n",
    "# Add climate files for a year within the main analysis range too (e.g., 2001)\n",
    "files_to_check.append(CLIMATE_DIR / f'mean_temp_{START_YEAR}.tif')\n",
    "files_to_check.append(CLIMATE_DIR / f'total_precip_{START_YEAR}.tif')\n",
    "\n",
    "\n",
    "# --- Run the check ---\n",
    "# This requires data to be downloaded and placed correctly.\n",
    "# If this cell fails, DO NOT proceed. Fix the data first.\n",
    "common_profile = verify_raster_alignment(files_to_check)\n",
    "\n",
    "assert common_profile is not None, \"Raster alignment failed. Stopping execution.\"\n",
    "\n",
    "# --- Store key dimensions for later use ---\n",
    "RASTER_HEIGHT = common_profile['height']\n",
    "RASTER_WIDTH = common_profile['width']\n",
    "RASTER_TRANSFORM = common_profile['transform']\n",
    "RASTER_CRS = common_profile['crs']\n",
    "\n",
    "print(f\"\\nCommon Raster Shape: ({RASTER_HEIGHT}, {RASTER_WIDTH})\")\n",
    "print(f\"Common CRS: {RASTER_CRS}\")\n",
    "print(f\"Common Transform: {RASTER_TRANSFORM}\")\n",
    "\n",
    "# --- Quick check on Landsat band count ---\n",
    "try:\n",
    "    with rasterio.open(example_landsat_path) as src:\n",
    "        if src.count != len(LANDSAT_BANDS):\n",
    "             print(f\"\\nWARNING: Expected {len(LANDSAT_BANDS)} Landsat bands based on LANDSAT_BANDS variable, but found {src.count} in {example_landsat_path.name}\")\n",
    "             print(f\"Actual band descriptions: {src.descriptions}\") # Requires descriptions set in GeoTIFF\n",
    "        else:\n",
    "             print(f\"\\nLandsat band count ({src.count}) matches expected count.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not perform Landsat band count check: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b99d20",
   "metadata": {},
   "source": [
    "## 4. Sampling Strategy\n",
    "\n",
    "For each year `t` from 2001 to 2021:\n",
    "1. Identify pixels where loss occurred exactly in year `t`.\n",
    "2. Identify pixels that are potential non-loss candidates (forested in 2000, no loss 2001-2021).\n",
    "3. Sample loss pixels.\n",
    "4. Sample `NON_LOSS_RATIO` times as many non-loss pixels randomly from the candidates.\n",
    "5. Store pixel coordinates (row, col) and associated target year/loss status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66277eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for sampled pixel coordinates and their corresponding year/loss status\n",
    "# Format: list of dicts [{'row': r, 'col': c, 'target_year': t, 'loss': 1}, ...]\n",
    "all_sampled_points = []\n",
    "\n",
    "print(\"Starting pixel sampling...\")\n",
    "sampling_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"Loading base data for sampling (Hansen Loss Year and Cover 2000)...\")\n",
    "    with rasterio.open(HANSEN_LOSS_YEAR_PATH) as loss_src:\n",
    "        # Verify shape matches common profile before reading\n",
    "        assert (loss_src.height, loss_src.width) == (RASTER_HEIGHT, RASTER_WIDTH), \"Loss year shape mismatch!\"\n",
    "        loss_year_data = loss_src.read(1)\n",
    "        print(f\"  Loaded Loss Year data ({loss_src.height}x{loss_src.width})\")\n",
    "\n",
    "    with rasterio.open(HANSEN_COVER_2000_PATH) as cover_src:\n",
    "        assert (cover_src.height, cover_src.width) == (RASTER_HEIGHT, RASTER_WIDTH), \"Cover shape mismatch!\"\n",
    "        cover_2000_data = cover_src.read(1)\n",
    "        print(\"  Loaded Tree Cover 2000 data\")\n",
    "\n",
    "    # --- Create mask for non-loss candidate pixels ---\n",
    "    # Condition 1: Sufficient tree cover in 2000\n",
    "    forest_mask = cover_2000_data >= MIN_TREE_COVER\n",
    "    # Condition 2: No loss recorded between 2001 and 2021 (loss year == 0 in Hansen)\n",
    "    no_loss_mask = loss_year_data == 0\n",
    "    # Combined mask for pixels eligible to be sampled as \"non-loss\" controls\n",
    "    non_loss_candidate_mask = forest_mask & no_loss_mask\n",
    "    # Get indices (row, col arrays) where mask is True\n",
    "    non_loss_candidate_indices = np.where(non_loss_candidate_mask)\n",
    "    num_non_loss_candidates = len(non_loss_candidate_indices[0])\n",
    "    print(f\"Found {num_non_loss_candidates} potential non-loss candidate pixels.\")\n",
    "\n",
    "    # Check if we have candidates to sample from\n",
    "    if num_non_loss_candidates == 0:\n",
    "        raise ValueError(\"No non-loss candidate pixels found based on criteria. Cannot sample.\")\n",
    "\n",
    "    # --- Sample pixels year by year ---\n",
    "    np.random.seed(RANDOM_STATE) # for reproducibility\n",
    "\n",
    "    for year_num, target_year in enumerate(YEARS):\n",
    "        start_time_year = time.time()\n",
    "        print(f\"\\n--- Sampling for target year: {target_year} ---\")\n",
    "\n",
    "        # 1. Find pixels with loss IN THIS target_year\n",
    "        # Hansen loss year codes 1-21 correspond to years 2001-2021\n",
    "        hansen_loss_code = target_year - 2000\n",
    "        loss_pixels_this_year_mask = loss_year_data == hansen_loss_code\n",
    "        loss_indices_this_year = np.where(loss_pixels_this_year_mask)\n",
    "        num_loss_pixels = len(loss_indices_this_year[0])\n",
    "        print(f\"  Found {num_loss_pixels} loss pixels.\")\n",
    "\n",
    "        if num_loss_pixels == 0:\n",
    "            print(\"  No loss pixels found for this year. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Add loss pixels to sample list\n",
    "        for r, c in zip(loss_indices_this_year[0], loss_indices_this_year[1]):\n",
    "            all_sampled_points.append({'row': r, 'col': c, 'target_year': target_year, 'loss': 1})\n",
    "\n",
    "        # 3. Sample non-loss pixels\n",
    "        # Calculate how many non-loss points to sample for this year\n",
    "        num_non_loss_to_sample = min(num_non_loss_candidates, num_loss_pixels * NON_LOSS_RATIO)\n",
    "        print(f\"  Sampling {num_non_loss_to_sample} non-loss pixels.\")\n",
    "\n",
    "        if num_non_loss_to_sample > 0:\n",
    "            # Randomly choose indices from the non_loss_candidate_indices array\n",
    "            # This samples indices *of the indices array*, not row/col directly\n",
    "            sampled_candidate_indices_idx = np.random.choice(\n",
    "                num_non_loss_candidates, num_non_loss_to_sample, replace=False\n",
    "            )\n",
    "            # Get the actual row and column values using these sampled indices\n",
    "            sampled_non_loss_rows = non_loss_candidate_indices[0][sampled_candidate_indices_idx]\n",
    "            sampled_non_loss_cols = non_loss_candidate_indices[1][sampled_candidate_indices_idx]\n",
    "\n",
    "            # Add non-loss pixels to sample list\n",
    "            for r, c in zip(sampled_non_loss_rows, sampled_non_loss_cols):\n",
    "                 # Target year is still the year we are comparing against\n",
    "                all_sampled_points.append({'row': r, 'col': c, 'target_year': target_year, 'loss': 0})\n",
    "\n",
    "        print(f\"  Finished sampling for {target_year}. Points this year: {num_loss_pixels + num_non_loss_to_sample}. Time: {time.time() - start_time_year:.2f}s\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    del loss_year_data, cover_2000_data, forest_mask, no_loss_mask, non_loss_candidate_mask\n",
    "    del loss_pixels_this_year_mask, loss_indices_this_year, non_loss_candidate_indices\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    # Shuffle the collected points for randomness if needed later (e.g., batch processing)\n",
    "    # Though temporal split doesn't strictly require shuffling here\n",
    "    all_sampled_points = shuffle(all_sampled_points, random_state=RANDOM_STATE)\n",
    "    print(f\"\\nTotal points sampled across all years: {len(all_sampled_points)}\")\n",
    "    print(f\"Sampling finished. Total time: {time.time() - sampling_start_time:.2f}s\")\n",
    "\n",
    "    # Optional: Quick check on class balance\n",
    "    loss_counts = pd.Series([p['loss'] for p in all_sampled_points]).value_counts()\n",
    "    print(f\"Sampled loss counts:\\n{loss_counts}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     print(f\"ERROR: Required file not found during sampling: {e}. Please ensure data exists.\")\n",
    "except ValueError as e:\n",
    "     print(f\"ERROR during sampling setup: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during sampling: {e}\")\n",
    "    # Depending on the error, you might want to stop execution\n",
    "    # raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af34867",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "Iterate through the `all_sampled_points`. For each point (row, col, target_year, loss):\n",
    "- Read pixel values from Landsat(target_year - 1).\n",
    "- Read pixel values from Climate(target_year - 1).\n",
    "- Read pixel values from Climate(target_year).\n",
    "- Read pixel values from DEM and Slope.\n",
    "- Store features and target in a structure suitable for Scikit-learn (e.g., NumPy array or Pandas DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589654fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for feature extraction\n",
    "features = []\n",
    "# Target 'y' is implicitly defined by the 'loss' key in all_sampled_points\n",
    "\n",
    "# --- Need raster dimensions and transform from verification step ---\n",
    "# RASTER_HEIGHT, RASTER_WIDTH, RASTER_TRANSFORM, RASTER_CRS are already defined if verification passed\n",
    "\n",
    "print(\"Starting feature extraction...\")\n",
    "extraction_start_time = time.time()\n",
    "\n",
    "# Cache for opened rasterio file handles\n",
    "open_files_cache = {}\n",
    "\n",
    "def get_file_handle(path, cache):\n",
    "    \"\"\"Gets or opens a rasterio file handle, caching it.\"\"\"\n",
    "    path_str = str(path) # Use string representation as key\n",
    "    if path_str not in cache:\n",
    "        if not path.exists():\n",
    "             raise FileNotFoundError(f\"Required file for feature extraction not found: {path_str}\")\n",
    "        print(f\"  Opening file: {path.name}\")\n",
    "        cache[path_str] = rasterio.open(path_str)\n",
    "    return cache[path_str]\n",
    "\n",
    "def get_pixel_value_at_rc(src, row, col):\n",
    "    \"\"\"Reads pixel value(s) for all bands at a specific row, col.\"\"\"\n",
    "    # Use Window(col_off, row_off, width, height)\n",
    "    window = Window(col, row, 1, 1)\n",
    "    # read() returns shape (bands, rows, cols) -> (bands, 1, 1)\n",
    "    # squeeze removes the singleton dimensions -> (bands,) or scalar if 1 band\n",
    "    return src.read(window=window).squeeze()\n",
    "\n",
    "# --- Process all sampled points ---\n",
    "extraction_errors = 0\n",
    "for i, point in enumerate(all_sampled_points):\n",
    "    row, col = point['row'], point['col']\n",
    "    t_year = point['target_year'] # Year loss status is defined for\n",
    "    t_minus_1 = t_year - 1      # Year for predictor data\n",
    "\n",
    "    if i > 0 and i % 20000 == 0: # Print progress periodically\n",
    "         elapsed_time = time.time() - extraction_start_time\n",
    "         points_per_sec = i / elapsed_time if elapsed_time > 0 else 0\n",
    "         print(f\"  Processed {i}/{len(all_sampled_points)} points... ({points_per_sec:.1f} points/sec)\")\n",
    "\n",
    "    # --- Feature dictionary for this point ---\n",
    "    # Start with static info to ensure it's always present if point is processed\n",
    "    point_features = {'row': row, 'col': col, 'target_year': t_year, 'loss': point['loss']}\n",
    "\n",
    "    try:\n",
    "        # --- Landsat (t-1) ---\n",
    "        lsat_path = LANDSAT_DIR / f'landsat_composite_{t_minus_1}.tif'\n",
    "        lsat_src = get_file_handle(lsat_path, open_files_cache)\n",
    "        lsat_values = get_pixel_value_at_rc(lsat_src, row, col)\n",
    "        # Check if return is scalar (single band) or array (multiple bands)\n",
    "        if lsat_values.ndim == 0: # Scalar if only one band read unexpectedly\n",
    "             if len(LANDSAT_BANDS) == 1:\n",
    "                 point_features[f'lsat_{LANDSAT_BANDS[0]}_{t_minus_1}'] = lsat_values\n",
    "             else:\n",
    "                 raise ValueError(f\"Read scalar Landsat value but expected {len(LANDSAT_BANDS)} bands.\")\n",
    "        else: # Should be an array of band values\n",
    "             if len(lsat_values) != len(LANDSAT_BANDS):\n",
    "                 raise ValueError(f\"Read {len(lsat_values)} Landsat bands but expected {len(LANDSAT_BANDS)}.\")\n",
    "             for band_name, band_value in zip(LANDSAT_BANDS, lsat_values):\n",
    "                 point_features[f'lsat_{band_name}_{t_minus_1}'] = band_value\n",
    "\n",
    "        # --- Climate (t-1) ---\n",
    "        temp_tm1_path = CLIMATE_DIR / f'mean_temp_{t_minus_1}.tif'\n",
    "        precip_tm1_path = CLIMATE_DIR / f'total_precip_{t_minus_1}.tif'\n",
    "        temp_tm1_src = get_file_handle(temp_tm1_path, open_files_cache)\n",
    "        precip_tm1_src = get_file_handle(precip_tm1_path, open_files_cache)\n",
    "        point_features[f'temp_{t_minus_1}'] = get_pixel_value_at_rc(temp_tm1_src, row, col)\n",
    "        point_features[f'precip_{t_minus_1}'] = get_pixel_value_at_rc(precip_tm1_src, row, col)\n",
    "\n",
    "        # --- Climate (t) ---\n",
    "        temp_t_path = CLIMATE_DIR / f'mean_temp_{t_year}.tif'\n",
    "        precip_t_path = CLIMATE_DIR / f'total_precip_{t_year}.tif'\n",
    "        temp_t_src = get_file_handle(temp_t_path, open_files_cache)\n",
    "        precip_t_src = get_file_handle(precip_t_path, open_files_cache)\n",
    "        point_features[f'temp_{t_year}'] = get_pixel_value_at_rc(temp_t_src, row, col)\n",
    "        point_features[f'precip_{t_year}'] = get_pixel_value_at_rc(precip_t_src, row, col)\n",
    "\n",
    "        # --- Static Vars ---\n",
    "        dem_src = get_file_handle(DEM_PATH, open_files_cache)\n",
    "        slope_src = get_file_handle(SLOPE_PATH, open_files_cache)\n",
    "        point_features['dem'] = get_pixel_value_at_rc(dem_src, row, col)\n",
    "        point_features['slope'] = get_pixel_value_at_rc(slope_src, row, col)\n",
    "\n",
    "        # --- Append the complete feature dictionary ---\n",
    "        features.append(point_features)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "         # This error should have been caught by get_file_handle but handle defensively\n",
    "         print(f\"WARNING: Skipping point {i} due to missing file: {e}\")\n",
    "         extraction_errors += 1\n",
    "         continue # Skip this point\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR extracting features for point {i} (row={row}, col={col}, target_year={t_year}): {type(e).__name__} - {e}\")\n",
    "        # Decide how to handle errors: skip point, fill with NaN? Here we skip.\n",
    "        extraction_errors += 1\n",
    "        continue\n",
    "\n",
    "# --- Close all opened files in the cache ---\n",
    "print(\"\\nClosing cached raster files...\")\n",
    "for path_str, src in open_files_cache.items():\n",
    "    try:\n",
    "        src.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing file {path_str}: {e}\")\n",
    "open_files_cache = {} # Clear the cache\n",
    "\n",
    "print(f\"Finished feature extraction. Total time: {time.time() - extraction_start_time:.2f}s\")\n",
    "print(f\"Processed {len(features)} points successfully.\")\n",
    "if extraction_errors > 0:\n",
    "    print(f\"Encountered {extraction_errors} errors during extraction (points skipped).\")\n",
    "\n",
    "# --- Convert list of dictionaries to DataFrame ---\n",
    "if not features:\n",
    "    raise SystemExit(\"No features were extracted successfully. Cannot proceed.\")\n",
    "\n",
    "print(\"\\nConverting extracted features to DataFrame...\")\n",
    "feature_df = pd.DataFrame(features)\n",
    "del features # Free up memory\n",
    "gc.collect()\n",
    "\n",
    "print(f\"DataFrame shape: {feature_df.shape}\")\n",
    "# Display sample data and check for initial NaNs which might indicate read errors or NoData values\n",
    "print(\"Sample data:\")\n",
    "print(feature_df.head())\n",
    "print(\"\\nCheck for missing values before cleaning:\")\n",
    "print(feature_df.isnull().sum())\n",
    "\n",
    "# --- Handle Missing Data (if any) ---\n",
    "# This could happen if rasters contained NoData values at sampled locations\n",
    "initial_rows = len(feature_df)\n",
    "feature_df = feature_df.dropna()\n",
    "final_rows = len(feature_df)\n",
    "\n",
    "if initial_rows != final_rows:\n",
    "    print(f\"\\nDropped {initial_rows - final_rows} rows containing NaN values.\")\n",
    "    print(f\"Final DataFrame shape after dropna: {feature_df.shape}\")\n",
    "else:\n",
    "    print(\"\\nNo rows dropped due to NaN values.\")\n",
    "\n",
    "if final_rows == 0:\n",
    "     raise SystemExit(\"All rows contained NaN values after extraction. Check input data quality.\")\n",
    "\n",
    "# --- Prepare final X and y for modeling ---\n",
    "target_series = feature_df['loss']\n",
    "year_series = feature_df['target_year'] # Keep track of year for temporal split\n",
    "# Drop non-feature columns (row, col might be useful later for mapping, keep if needed)\n",
    "X = feature_df.drop(columns=['row', 'col', 'target_year', 'loss'])\n",
    "y = target_series\n",
    "\n",
    "# Store feature names in the order they appear in X for later use (importance plot)\n",
    "FEATURE_NAMES = X.columns.tolist()\n",
    "\n",
    "print(\"\\nFinal feature columns (X):\", FEATURE_NAMES)\n",
    "print(\"Target distribution (y):\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf682b1",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Train/Test Split (Temporal): TODO\n",
    "\n",
    "Split the data based on the `target_year`. Data from years before `TEST_SPLIT_YEAR` will be used for training, and data from `TEST_SPLIT_YEAR` onwards for testing. This prevents data leakage from the future into the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85683176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Splitting data temporally based on year {TEST_SPLIT_YEAR}...\")\n",
    "\n",
    "# Use the 'year_series' created alongside X and y\n",
    "train_mask = year_series < TEST_SPLIT_YEAR\n",
    "test_mask = year_series >= TEST_SPLIT_YEAR\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "X_test = X[test_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "# Keep track of years for verification (optional)\n",
    "# train_years = year_series[train_mask]\n",
    "# test_years = year_series[test_mask]\n",
    "\n",
    "# Cleanup intermediate objects if memory is tight\n",
    "# del X, y, feature_df, year_series, train_mask, test_mask\n",
    "# gc.collect()\n",
    "\n",
    "print(f\"Train set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set shape: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Verify the split worked as expected\n",
    "if not X_train.empty and not X_test.empty:\n",
    "    print(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(f\"Test target distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "    # print(f\"Train years range: {train_years.min()} - {train_years.max()}\")\n",
    "    # print(f\"Test years range: {test_years.min()} - {test_years.max()}\")\n",
    "else:\n",
    "    print(\"WARNING: Train or Test set is empty after split. Check TEST_SPLIT_YEAR and data distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8883b8",
   "metadata": {},
   "source": [
    "## 7. Model Training (Random Forest)\n",
    "\n",
    "Train a Random Forest Classifier on the training data. We use `class_weight='balanced'` to help handle potential class imbalance between loss and no-loss pixels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e055b86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training data exists\n",
    "if X_train.empty or y_train.empty:\n",
    "    raise SystemExit(\"Training data is empty. Cannot train model.\")\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "train_start_time = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=RF_N_ESTIMATORS,\n",
    "    max_depth=RF_MAX_DEPTH,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=RF_N_JOBS,             # Use all available cores\n",
    "    class_weight='balanced',      # Adjust for class imbalance\n",
    "    oob_score=False               # Set to True to estimate generalization score without test set (slower)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Finished training. Total time: {time.time() - train_start_time:.2f}s\")\n",
    "\n",
    "# Optional: Print OOB score if calculated\n",
    "# if rf_classifier.oob_score:\n",
    "#     print(f\"Out-of-Bag (OOB) Score: {rf_classifier.oob_score_:.4f}\")\n",
    "\n",
    "# --- Save the trained model ---\n",
    "model_filename = OUTPUT_DIR / f'rf_forest_loss_model_{START_YEAR}_{END_YEAR}.joblib'\n",
    "try:\n",
    "    joblib.dump(rf_classifier, model_filename)\n",
    "    print(f\"Model saved successfully to: {model_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688921c",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Evaluate the trained model's performance on the temporally independent test set using various classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if test data exists\n",
    "if X_test.empty or y_test.empty:\n",
    "    print(\"Test data is empty. Skipping evaluation.\")\n",
    "else:\n",
    "    print(\"Evaluating model on the test set...\")\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1] # Probability of class 1 (loss)\n",
    "\n",
    "    # --- Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Use zero_division=0 to avoid warnings if a class has no predicted samples\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Loss', 'Loss'], zero_division=0))\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    try:\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Loss', 'Loss'])\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate Confusion Matrix plot: {e}\")\n",
    "\n",
    "\n",
    "    # --- ROC AUC Score ---\n",
    "    # Check if both classes are present in y_test for ROC AUC calculation\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "        # Optional: Plot ROC Curve\n",
    "        try:\n",
    "            from sklearn.metrics import RocCurveDisplay\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            RocCurveDisplay.from_predictions(y_test, y_pred_proba, ax=ax, name='Random Forest')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate ROC Curve plot: {e}\")\n",
    "    else:\n",
    "        print(\"\\nROC AUC Score cannot be calculated: Only one class present in the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e6418",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Analyze the feature importances provided by the Random Forest model (based on mean decrease in impurity - Gini importance) to understand which variables were most influential in the model's predictions.g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating and plotting Feature Importances...\")\n",
    "\n",
    "try:\n",
    "    importances = rf_classifier.feature_importances_\n",
    "    # Get standard deviations of importances across trees (optional, adds compute time)\n",
    "    # std = np.std([tree.feature_importances_ for tree in rf_classifier.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1] # Sort features by importance (descending)\n",
    "\n",
    "    # --- Create DataFrame for easier plotting/viewing ---\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': [FEATURE_NAMES[i] for i in indices],\n",
    "        'Importance': importances[indices],\n",
    "        # 'StdDev': std[indices] # Uncomment if std is calculated\n",
    "    })\n",
    "\n",
    "    print(\"\\nTop 20 Feature Importances:\")\n",
    "    print(importance_df.head(20))\n",
    "\n",
    "    # --- Plot Feature Importances ---\n",
    "    N_FEATURES_TO_PLOT = 20\n",
    "    plt.figure(figsize=(12, max(6, N_FEATURES_TO_PLOT // 2))) # Adjust height based on number of features\n",
    "    plt.title(f\"Feature Importances (Top {N_FEATURES_TO_PLOT})\")\n",
    "    plt.barh(range(N_FEATURES_TO_PLOT), # Use barh for horizontal plot\n",
    "             importance_df['Importance'].head(N_FEATURES_TO_PLOT)[::-1], # Plot descending importance\n",
    "             # xerr=importance_df['StdDev'].head(N_FEATURES_TO_PLOT)[::-1], # Uncomment if std is calculated\n",
    "             align='center')\n",
    "    plt.yticks(range(N_FEATURES_TO_PLOT),\n",
    "               importance_df['Feature'].head(N_FEATURES_TO_PLOT)[::-1]) # Labels for y-axis\n",
    "    plt.xlabel(\"Mean Decrease in Impurity (Gini Importance)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim([-1, N_FEATURES_TO_PLOT])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Save importance data\n",
    "    importance_filename = OUTPUT_DIR / f'rf_feature_importances_{START_YEAR}_{END_YEAR}.csv'\n",
    "    importance_df.to_csv(importance_filename, index=False)\n",
    "    print(f\"Feature importances saved to: {importance_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during feature importance calculation or plotting: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de410a1d",
   "metadata": {},
   "source": [
    "## 10. (Optional) Prediction Map Visualization\n",
    "\n",
    "This section provides a function to generate a spatial map of predicted loss probability for a given year using the trained model. This is computationally intensive and requires significant memory/time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9144a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This section is computationally intensive ---\n",
    "\n",
    "def predict_loss_probability_map(model, feature_names_ordered, output_path, year_to_predict):\n",
    "    \"\"\"Generates a GeoTIFF map of predicted loss probability for a given year.\"\"\"\n",
    "    print(f\"\\nGenerating loss probability map for year: {year_to_predict}\")\n",
    "    prediction_start_time = time.time()\n",
    "    t_minus_1 = year_to_predict - 1\n",
    "\n",
    "    # --- Define input file paths for the prediction year ---\n",
    "    lsat_path = LANDSAT_DIR / f'landsat_composite_{t_minus_1}.tif'\n",
    "    temp_tm1_path = CLIMATE_DIR / f'mean_temp_{t_minus_1}.tif'\n",
    "    precip_tm1_path = CLIMATE_DIR / f'total_precip_{t_minus_1}.tif'\n",
    "    temp_t_path = CLIMATE_DIR / f'mean_temp_{year_to_predict}.tif'\n",
    "    precip_t_path = CLIMATE_DIR / f'total_precip_{year_to_predict}.tif'\n",
    "    dem_path_pred = DEM_PATH # Static path\n",
    "    slope_path_pred = SLOPE_PATH # Static path\n",
    "\n",
    "    # Check if all required files exist before starting\n",
    "    required_files = {\n",
    "        'lsat': lsat_path, 'temp_tm1': temp_tm1_path, 'prec_tm1': precip_tm1_path,\n",
    "        'temp_t': temp_t_path, 'prec_t': precip_t_path, 'dem': dem_path_pred, 'slope': slope_path_pred\n",
    "    }\n",
    "    missing_files = [name for name, p in required_files.items() if not p.exists()]\n",
    "    if missing_files:\n",
    "        print(f\"ERROR: Missing required raster files for prediction year {year_to_predict}: {missing_files}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Open source files and get metadata from a reference (e.g., Landsat)\n",
    "        handles = {name: rasterio.open(p) for name, p in required_files.items()}\n",
    "        ref_src = handles['lsat']\n",
    "        profile = ref_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1, nodata=-9999.0) # Output is probability (float)\n",
    "\n",
    "        # Create the output file\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            # Define processing blocks (adjust block size based on available RAM)\n",
    "            block_shape = (512, 512) # Larger block might be faster if RAM allows\n",
    "            total_blocks = len(list(dst.block_windows(block_shape)))\n",
    "            processed_blocks = 0\n",
    "\n",
    "            print(f\"Processing {total_blocks} blocks...\")\n",
    "            for block_info, window in dst.block_windows(block_shape):\n",
    "                processed_blocks += 1\n",
    "                if processed_blocks % max(1, total_blocks // 10) == 0: # Print progress roughly 10 times\n",
    "                    print(f\"  Processing block {processed_blocks}/{total_blocks}...\")\n",
    "\n",
    "                # Read data for the block from all sources\n",
    "                block_data = {}\n",
    "                # Read Landsat (all bands)\n",
    "                block_data['lsat'] = handles['lsat'].read(window=window)\n",
    "                # Read single band rasters\n",
    "                for name in ['temp_tm1', 'prec_tm1', 'temp_t', 'prec_t', 'dem', 'slope']:\n",
    "                    block_data[name] = handles[name].read(1, window=window)\n",
    "\n",
    "                block_h, block_w = block_data['lsat'].shape[1], block_data['lsat'].shape[2]\n",
    "                if block_h == 0 or block_w == 0: continue # Skip empty blocks\n",
    "                n_pixels_in_block = block_h * block_w\n",
    "\n",
    "                # --- Assemble features for the block ---\n",
    "                # Important: Feature order must match training order (FEATURE_NAMES)\n",
    "                block_features_list = []\n",
    "                for feature_name in feature_names_ordered:\n",
    "                    if feature_name.startswith('lsat_'):\n",
    "                        # Extract band name and check year (should be t-1)\n",
    "                        parts = feature_name.split('_')\n",
    "                        band_name = parts[1]\n",
    "                        try:\n",
    "                            band_index = LANDSAT_BANDS.index(band_name)\n",
    "                            feature_data = block_data['lsat'][band_index].ravel()\n",
    "                        except ValueError:\n",
    "                            raise ValueError(f\"Unknown Landsat band '{band_name}' in feature name list.\")\n",
    "                    elif feature_name.startswith('temp_'):\n",
    "                        year_suffix = feature_name.split('_')[1]\n",
    "                        if year_suffix == str(t_minus_1):\n",
    "                            feature_data = block_data['temp_tm1'].ravel()\n",
    "                        elif year_suffix == str(year_to_predict):\n",
    "                            feature_data = block_data['temp_t'].ravel()\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unexpected year suffix in temp feature: {feature_name}\")\n",
    "                    elif feature_name.startswith('precip_'):\n",
    "                        year_suffix = feature_name.split('_')[1]\n",
    "                        if year_suffix == str(t_minus_1):\n",
    "                            feature_data = block_data['prec_tm1'].ravel()\n",
    "                        elif year_suffix == str(year_to_predict):\n",
    "                            feature_data = block_data['prec_t'].ravel()\n",
    "                        else:\n",
    "                             raise ValueError(f\"Unexpected year suffix in precip feature: {feature_name}\")\n",
    "                    elif feature_name == 'dem':\n",
    "                        feature_data = block_data['dem'].ravel()\n",
    "                    elif feature_name == 'slope':\n",
    "                        feature_data = block_data['slope'].ravel()\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown feature name structure: {feature_name}\")\n",
    "                    block_features_list.append(feature_data)\n",
    "\n",
    "                # Combine into (n_pixels, n_features) array\n",
    "                block_features_array = np.vstack(block_features_list).T\n",
    "\n",
    "                # --- Handle potential NoData/NaN values ---\n",
    "                # Check for NaNs across all features for each pixel\n",
    "                valid_pixel_mask = ~np.isnan(block_features_array).any(axis=1)\n",
    "                # Create output block array, fill with NoData initially\n",
    "                out_block = np.full(n_pixels_in_block, profile['nodata'], dtype=np.float32)\n",
    "\n",
    "                # Predict only for valid pixels\n",
    "                if np.any(valid_pixel_mask):\n",
    "                    valid_features = block_features_array[valid_pixel_mask, :]\n",
    "                    # Predict probability of class 1 (loss)\n",
    "                    pred_proba = model.predict_proba(valid_features)[:, 1]\n",
    "                    # Fill predictions into the output block based on the valid mask\n",
    "                    out_block[valid_pixel_mask] = pred_proba\n",
    "\n",
    "                # Reshape back to 2D block and write\n",
    "                out_block = out_block.reshape(block_h, block_w)\n",
    "                dst.write(out_block.astype(rasterio.float32), 1, window=window)\n",
    "\n",
    "        # Close all source file handles\n",
    "        for handle in handles.values():\n",
    "            handle.close()\n",
    "\n",
    "        print(f\"Probability map saved to: {output_path}\")\n",
    "        print(f\"Prediction map generation time: {time.time() - prediction_start_time:.2f}s\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR generating prediction map: Input file not found - {e}\")\n",
    "    except MemoryError:\n",
    "        print(\"ERROR generating prediction map: Insufficient memory. Try smaller block_shape.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR generating prediction map: {type(e).__name__} - {e}\")\n",
    "\n",
    "\n",
    "# --- Example Call (run only if needed and model is trained) ---\n",
    "# Check if model and feature names exist before running\n",
    "# if 'rf_classifier' in locals() and 'FEATURE_NAMES' in locals() and Path(model_filename).exists():\n",
    "#     PREDICTION_YEAR = 2021 # Choose a year from the test set range\n",
    "#     prob_map_filename = OUTPUT_DIR / f'rf_loss_probability_{PREDICTION_YEAR}.tif'\n",
    "#     # Optional: Load model if kernel restarted\n",
    "#     # print(f\"Loading model from {model_filename}...\")\n",
    "#     # loaded_model = joblib.load(model_filename)\n",
    "#     # predict_loss_probability_map(loaded_model, FEATURE_NAMES, prob_map_filename, PREDICTION_YEAR)\n",
    "#     # OR use the model in memory if kernel wasn't restarted:\n",
    "#     predict_loss_probability_map(rf_classifier, FEATURE_NAMES, prob_map_filename, PREDICTION_YEAR)\n",
    "# else:\n",
    "#     print(\"\\nModel or feature names not available. Skipping prediction map generation.\")\n",
    "#     if not 'rf_classifier' in locals(): print(\" Reason: 'rf_classifier' not found.\")\n",
    "#     if not 'FEATURE_NAMES' in locals(): print(\" Reason: 'FEATURE_NAMES' not found.\")\n",
    "#     if not Path(model_filename).exists(): print(f\" Reason: Model file not found at {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7e88a",
   "metadata": {},
   "source": [
    "## 11. Conclusion & Next Steps\n",
    "\n",
    "- Summarize findings from evaluation metrics (e.g., Accuracy, Precision, Recall, F1, ROC AUC) and the feature importance analysis. Which factors were most predictive of forest loss in this region according to the model?\n",
    "- Discuss limitations:\n",
    "    - **Correlation vs Causation:** The model identifies correlations, not necessarily causal links.\n",
    "    - **Data Resolution:** 30m resolution might miss fine-scale drivers. Climate data resolution (TerraClimate ~4km) is much coarser than Landsat/Hansen.\n",
    "    - **Loss Type:** Hansen data aggregates loss from various causes (logging, pests, wind, etc.). The model doesn't differentiate these.\n",
    "    - **Sampling Bias:** The non-loss sampling strategy might influence results.\n",
    "    - **Spatial Autocorrelation:** This analysis treats pixels independently, ignoring spatial relationships which are likely important in forest dynamics.\n",
    "    - **Model Simplicity:** Random Forest is robust but might not capture complex spatio-temporal interactions as well as more advanced models (if data/time permitted).\n",
    "- Suggest potential future work:\n",
    "    - Hyperparameter tuning of the Random Forest model (e.g., using GridSearchCV or RandomizedSearchCV).\n",
    "    - Exploring other models (e.g., Gradient Boosting Machines like XGBoost/LightGBM, potentially simpler deep learning if data is very large).\n",
    "    - Incorporating spatial features (e.g., distance to roads/mills, patch metrics, neighborhood characteristics).\n",
    "    - Using focal statistics on predictor variables to capture neighborhood context.\n",
    "    - Attempting to differentiate loss types if finer-grained data becomes available.\n",
    "    - Analyzing model predictions spatially - where does the model perform well/poorly?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
